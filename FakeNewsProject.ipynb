{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Project\n",
    "The goal of this project is to create a fake news prediction system. Fake news is a major problem that can have serious negative effects on how people understand the world around them. You will work with a dataset containing real and fake news in order to train a simple and a more advanced classifier to solve this problem. This project covers the full Data Science pipeline, from data processing, to modelling, to visualization and interpretation.\n",
    "## Part 1 Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"news_sample.csv\")\n",
    "dfcpy = df.copy()\n",
    "dfcpy = dfcpy.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.probability import FreqDist\n",
    "from cleantext import clean\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "  clean_text = re.sub(r'([A-Z][A-z]+.?) ([0-9]{1,2}?), ([0-9]{4})', '<DATE>', text)\n",
    "  clean_text = clean(clean_text,\n",
    "    lower=True,\n",
    "    no_urls=True, replace_with_url=\"<URL>\",\n",
    "    no_emails=True, replace_with_email=\"<EMAIL>\",\n",
    "    no_numbers=True, replace_with_number= r\"<NUM>\",\n",
    "    no_currency_symbols=True, replace_with_currency_symbol=\"<CUR>\",\n",
    "    no_punct=True, replace_with_punct=\"\",\n",
    "    no_line_breaks=True \n",
    "  )\n",
    "  return clean_text\n",
    "\n",
    "def rmv_stopwords(tokens):\n",
    "  stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "  tokens = [word for word in tokens if word not in stop_words]\n",
    "  return tokens\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "  stemmer=PorterStemmer()\n",
    "  Output=[stemmer.stem(word) for word in tokens]\n",
    "  return Output\n",
    "\n",
    "# build a vocabulary from a dataframe with list of tokens\n",
    "def build_vocabulary(df_tokens):\n",
    "    # Flatten the list of tokens\n",
    "  tokens = []\n",
    "  for lst in df_tokens:\n",
    "    tokens += lst\n",
    "  fq = FreqDist(tokens)\n",
    "  return fq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning:\n",
      "vocabulary size: 16577\n",
      "\n",
      "After removing stopwords:\n",
      "vocabulary size: 16445\n",
      "reduction rate of the vocabulary size: 132 words\n",
      "\n",
      "After stemming:\n",
      "vocabulary size: 11031\n",
      "reduction rate of the vocabulary size: 5414 words\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv(file)\n",
    "dfcpy = df.copy()\n",
    "\n",
    "dfcpy.content = dfcpy.content.apply(clean_text)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'<[\\w]+>|[\\w]+')\n",
    "dfcpy[\"tokenized\"] = dfcpy.content.apply(tokenizer.tokenize)\n",
    "\n",
    "vocab = build_vocabulary(dfcpy.tokenized)\n",
    "vocab_size = vocab.B()\n",
    "vocab.\n",
    "print(\"After cleaning:\")\n",
    "print(f\"vocabulary size: {vocab_size}\\n\")\n",
    "\n",
    "dfcpy.tokenized = dfcpy.tokenized.apply(rmv_stopwords)\n",
    "vocab = build_vocabulary(dfcpy.tokenized)\n",
    "# reduction rate of the vocabulary size\n",
    "reduction = vocab_size - vocab.B()\n",
    "vocab_size = vocab.B()\n",
    "print(\"After removing stopwords:\")\n",
    "print(f\"vocabulary size: {vocab_size}\")\n",
    "print(f\"reduction rate of the vocabulary size: {reduction} words\\n\")\n",
    "\n",
    "dfcpy.tokenized = dfcpy.tokenized.apply(stem_tokens)\n",
    "vocab = build_vocabulary(dfcpy.tokenized)\n",
    "reduction = vocab_size - vocab.B()\n",
    "vocab_size = vocab.B()\n",
    "print(\"After stemming:\")\n",
    "print(f\"vocabulary size: {vocab_size}\")\n",
    "print(f\"reduction rate of the vocabulary size: {reduction} words\\n\")\n",
    "\n",
    "# make tokenize colum into a string with whitespace separator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. counting the number of URLs in the content\n",
    "2. counting the number of dates in the content\n",
    "3. counting the number of numeric values in the content\n",
    "4. determining the 100 more frequent words that appear in the content\n",
    "5. plot the frequency of the 10000 most frequent words (any interesting patterns?)\n",
    "6. run the analysis in point 4 and 5 both before and after removing stopwords and applying stemming: do you see any difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# import dask.dataframe as dd\n",
    "\n",
    "# df = pd.read_csv(\"995,000_rows.csv\", usecols=['id','content', 'type', 'url', 'title', 'authors', 'domain'], engine='c', dtype = str)\n",
    "# dfcpy = df.copy()\n",
    "# dfcpy = dfcpy.dropna(subset=['id'])\n",
    "# dfcpy = dfcpy.dropna(subset=['content'])\n",
    "# dfcpy = dfcpy.dropna(subset=['type'])\n",
    "# ddf = dd.from_pandas(dfcpy, npartitions=10) # find your own number of partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plot the frequency of the top n words\n",
    "def plot_freq(fq, top_n):\n",
    "  common_words = fq.most_common(top_n)\n",
    "  fq.most\n",
    "  # convert the list of tuples to a dictionary \n",
    "  all_freq = dict(common_words)\n",
    "  # create a plot\n",
    "  # plot most be less than 2^16 pixels in each direction\n",
    "  plt.figure(figsize = (top_n*0.1, 5))\n",
    "  plt.xticks(rotation = 90,fontsize = 5)\n",
    "  plt.yticks(range(0, max(all_freq.values())+1, 300))\n",
    "  sns.lineplot(x = list(all_freq.keys()), y = list(all_freq.values()), color = 'red')\n",
    "  sns.barplot(x = list(all_freq.keys()), y = list(all_freq.values()))\n",
    "  plt.title(f'Top {top_n} most common words')\n",
    "  plt.xlabel('Words')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.grid(axis = 'y')\n",
    "  plt.show()\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plot the frequency of the top n words\n",
    "def plot_freq1(fq, top_n):\n",
    "  common_words = fq.most_common(top_n)\n",
    "  # convert the list of tuples to a dictionary \n",
    "  all_freq = dict(common_words)\n",
    "  # create a plot\n",
    "  plt.figure(figsize=(5, 16))\n",
    "  plt.xticks(rotation=90,)\n",
    "  sns.barplot(x = list(all_freq.values()), y = list(all_freq.keys()))\n",
    "  plt.title(f'Top {top_n} most common words')\n",
    "  plt.xlabel('Frequency')\n",
    "  plt.ylabel('Words')\n",
    "  plt.show()\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddf_update = ddf.content.apply(clean_text).compute()\n",
    "dfcpy.content = dfcpy.content.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'<[\\w]+>|[\\w]+')\n",
    "dfcpy[\"tokenized\"] = dfcpy.content.apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcpy.tokenized = dfcpy.tokenized.apply(rmv_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcpy.tokenized = dfcpy.tokenized.apply(stem_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocabulary(dfcpy.tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcpy.tokenized = dfcpy.tokenized.apply(' '.join)\n",
    "dfcpy.to_csv('cleaned_news_sample.csv', index=False)\n",
    "\n",
    "url_freq = vocab.get(\"<url>\",0)\n",
    "date_freq = vocab.get(\"<date>\",0)\n",
    "num_freq = vocab.get(\"<num>\",0)\n",
    "print(f\"Number of URLs in the content: {url_freq}\")\n",
    "print(f\"Number of dates in the content: {date_freq}\")\n",
    "print(f\"Number of numerics in the content: {num_freq}\")\n",
    "plot_freq(vocab, 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set:\n",
      "79     past <num> month other repeatedli warn isi pos...\n",
      "161    trend global connect enterpris market <num> an...\n",
      "112    <num> principl govern success peopl headlin bi...\n",
      "109    who poster talk headlin bitcoin blockchain sea...\n",
      "101    son god goe forth war reader think stori fact ...\n",
      "Name: tokenized, dtype: object\n",
      "\n",
      "Validation Set:\n",
      "154    fed seiz <cur> <num> million bitcoin alleg sil...\n",
      "153    obama lawless presid us histori titl bush head...\n",
      "119    easili understand differ day christ day lord e...\n",
      "104    play hitler card headlin bitcoin blockchain se...\n",
      "30     greg hunter big bank big troubl syrianorth kor...\n",
      "Name: tokenized, dtype: object\n",
      "\n",
      "Test Set:\n",
      "137    hubbl captur incred rare imag explod star read...\n",
      "214    disturb sugar daddysugar babi relationship don...\n",
      "148    worldwid laser technolog market analysi report...\n",
      "82     interview princ harri septemb former presid ba...\n",
      "73     richmond fed suffer biggest <num> month drop s...\n",
      "Name: tokenized, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('cleaned_news_sample.csv')\n",
    "dfcpy = df.copy()\n",
    "\n",
    "# TODO: should I split the tokenized column into a list of words?\n",
    "# dfcpy['tokenized'] = dfcpy['tokenized'].apply(lambda x: x.split())\n",
    "\n",
    "X = dfcpy['tokenized']\n",
    "y = dfcpy['type']\n",
    "\n",
    "train_ratio = 0.80\n",
    "validation_ratio = 0.10\n",
    "test_ratio = 0.10\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 1 - train_ratio, random_state = 42) \n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size= test_ratio / (test_ratio + validation_ratio), random_state = 42)\n",
    "\n",
    "print(\"Training Set:\")\n",
    "print(x_train.head())\n",
    "\n",
    "print(\"\\nValidation Set:\")\n",
    "print(x_val.head())\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(x_test.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: A simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 8233)\t0.018634608315950867\n",
      "  (0, 3838)\t0.04953985673130612\n",
      "  (0, 8331)\t0.05350026514226691\n",
      "  (0, 8503)\t0.03880908542344456\n",
      "  (0, 4178)\t0.028078314115583005\n",
      "  (0, 7800)\t0.05350026514226691\n",
      "  (0, 2998)\t0.044550332216270805\n",
      "  (0, 3555)\t0.02399805672888513\n",
      "  (0, 4993)\t0.05350026514226691\n",
      "  (0, 5895)\t0.04953985673130612\n",
      "  (0, 9200)\t0.044550332216270805\n",
      "  (0, 7019)\t0.10700053028453382\n",
      "  (0, 8329)\t0.07761817084688911\n",
      "  (0, 1570)\t0.05350026514226691\n",
      "  (0, 6911)\t0.03521730826924733\n",
      "  (0, 3631)\t0.033819560908409256\n",
      "  (0, 6522)\t0.04126381643643715\n",
      "  (0, 9041)\t0.033819560908409256\n",
      "  (0, 1467)\t0.039959539348465405\n",
      "  (0, 8511)\t0.05350026514226691\n",
      "  (0, 3909)\t0.033819560908409256\n",
      "  (0, 5697)\t0.016210496666516805\n",
      "  (0, 4698)\t0.02964447201004746\n",
      "  (0, 2772)\t0.037546315433279076\n",
      "  (0, 5641)\t0.0310096064224693\n",
      "  :\t:\n",
      "  (174, 6670)\t0.0838684910594962\n",
      "  (174, 2849)\t0.07855811120528691\n",
      "  (174, 4214)\t0.06959456210324742\n",
      "  (174, 4713)\t0.0704465373491803\n",
      "  (174, 8742)\t0.2535976921468615\n",
      "  (174, 5418)\t0.04902971758536874\n",
      "  (174, 4886)\t0.044020309745950274\n",
      "  (174, 5467)\t0.03560776387505284\n",
      "  (174, 7828)\t0.03740358929906932\n",
      "  (174, 8467)\t0.03475578862911996\n",
      "  (174, 2808)\t0.03835203630593122\n",
      "  (174, 7248)\t0.037873338730375225\n",
      "  (174, 937)\t0.07670407261186243\n",
      "  (174, 902)\t0.038111547744790336\n",
      "  (174, 3681)\t0.03763736645334406\n",
      "  (174, 8233)\t0.04794476365481746\n",
      "  (174, 9200)\t0.11462302360411661\n",
      "  (174, 3186)\t0.06797265337929455\n",
      "  (174, 7278)\t0.10605430122423315\n",
      "  (174, 6088)\t0.3354739642379848\n",
      "  (174, 8938)\t0.1105854509348779\n",
      "  (174, 8984)\t0.05055326664223082\n",
      "  (174, 1617)\t0.052597552751679405\n",
      "  (174, 5774)\t0.0650126640628748\n",
      "  (174, 5587)\t0.06212399872095017\n",
      "Accuracy: 0.8181818181818182\n",
      "Recall: 1.0\n",
      "Precision: 0.8181818181818182\n",
      "F!: 0.9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# df = pd.read_csv('cleaned_news_sample.csv')\n",
    "# dfcpy = df.copy()\n",
    "\n",
    "# give each row a true/false column depending on if the type is fake or not using map\n",
    "dfcpy['fake'] = dfcpy['type'].map({'fake': 1, 'conspiracy': 1, 'junksci': 1, 'clickbait': 0, 'political': 0, 'reliable': 0})\n",
    "dfcpy = dfcpy.dropna(subset=['fake'])\n",
    "dfcpy['fake'] = dfcpy['fake'].astype(int)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(x_train)\n",
    "X_test_vectorized = vectorizer.transform(x_test)\n",
    "x_val_vectorized = vectorizer.transform(x_val)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "y_pred = model.predict(x_val_vectorized)\n",
    "\n",
    "# score the model\n",
    "import sklearn.metrics as metrics\n",
    "accuracy_score = metrics.accuracy_score(y_val, y_pred)\n",
    "recall_score = metrics.recall_score(y_val, y_pred, average='binary', pos_label = 'fake')\n",
    "precision_score = metrics.precision_score(y_val, y_pred, average='binary', pos_label = 'fake')\n",
    "f1_score = metrics.f1_score(y_val, y_pred, average='binary', pos_label = 'fake')\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score}\")\n",
    "print(f\"Recall: {recall_score}\")\n",
    "print(f\"Precision: {precision_score}\")  \n",
    "print(f\"F!: {f1_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_enviroment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
