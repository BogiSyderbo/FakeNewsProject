{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Project\n",
    "The goal of this project is to create a fake news prediction system. Fake news is a major problem that can have serious negative effects on how people understand the world around them. You will work with a dataset containing real and fake news in order to train a simple and a more advanced classifier to solve this problem. This project covers the full Data Science pipeline, from data processing, to modelling, to visualization and interpretation.\n",
    "## Part 1 Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(\"news_sample.csv\")\n",
    "dfcpy = df.copy()\n",
    "dfcpy = dfcpy.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.probability import FreqDist\n",
    "from cleantext import clean\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "  clean_text = re.sub(r'([A-Z][A-z]+.?) ([0-9]{1,2}?), ([0-9]{4})', '<DATE>', text)\n",
    "  clean_text = clean(clean_text,\n",
    "    lower=True,\n",
    "    no_urls=True, replace_with_url=\"<URL>\",\n",
    "    no_emails=True, replace_with_email=\"<EMAIL>\",\n",
    "    no_numbers=True, replace_with_number= r\"<NUM>\",\n",
    "    no_currency_symbols=True, replace_with_currency_symbol=\"<CUR>\",\n",
    "    no_punct=True, replace_with_punct=\"\",\n",
    "    no_line_breaks=True \n",
    "  )\n",
    "  return clean_text\n",
    "\n",
    "def rmv_stopwords(tokens):\n",
    "  stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "  tokens = [word for word in tokens if word not in stop_words]\n",
    "  return tokens\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "  stemmer=PorterStemmer()\n",
    "  Output=[stemmer.stem(word) for word in tokens]\n",
    "  return Output\n",
    "\n",
    "# build a vocabulary from a dataframe with list of tokens\n",
    "def build_vocabulary(df_tokens):\n",
    "    # Flatten the list of tokens\n",
    "  tokens = []\n",
    "  for lst in df_tokens:\n",
    "    tokens += lst\n",
    "  fq = FreqDist(tokens)\n",
    "  return fq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcpy = df.copy()\n",
    "\n",
    "dfcpy.content = dfcpy.content.apply(clean_text)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'<[\\w]+>|[\\w]+')\n",
    "dfcpy[\"tokenized\"] = dfcpy.content.apply(tokenizer.tokenize)\n",
    "\n",
    "vocab = build_vocabulary(dfcpy.tokenized)\n",
    "vocab_size = vocab.B()\n",
    "print(\"After cleaning:\")\n",
    "print(f\"vocabulary size: {vocab_size}\\n\")\n",
    "\n",
    "dfcpy.tokenized = dfcpy.tokenized.apply(rmv_stopwords)\n",
    "vocab = build_vocabulary(dfcpy.tokenized)\n",
    "# reduction rate of the vocabulary size\n",
    "reduction = ((vocab_size - vocab.B())/vocab_size)*100\n",
    "vocab_size = vocab.B()\n",
    "print(\"After removing stopwords:\")\n",
    "print(f\"vocabulary size: {vocab_size}\")\n",
    "print(f\"reduction rate of the vocabulary size: {reduction:.2f}%\\n\")\n",
    "\n",
    "dfcpy.tokenized = dfcpy.tokenized.apply(stem_tokens)\n",
    "vocab = build_vocabulary(dfcpy.tokenized)\n",
    "reduction = ((vocab_size - vocab.B())/vocab_size)*100\n",
    "vocab_size = vocab.B()\n",
    "print(\"After stemming:\")\n",
    "print(f\"vocabulary size: {vocab_size}\")\n",
    "print(f\"reduction rate of the vocabulary size: {reduction:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. counting the number of URLs in the content\n",
    "2. counting the number of dates in the content\n",
    "3. counting the number of numeric values in the content\n",
    "4. determining the 100 more frequent words that appear in the content\n",
    "5. plot the frequency of the 10000 most frequent words (any interesting patterns?)\n",
    "6. run the analysis in point 4 and 5 both before and after removing stopwords and applying stemming: do you see any difference?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install \"modin[ray]\" (only works in python 3.10)\n",
    "import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"995,000_rows.csv\", usecols=['content', 'type', 'url', 'title', 'authors', 'domain'], engine='c', dtype = str, nrows=70000)\n",
    "dfcpy = df.copy()\n",
    "dfcpy = dfcpy.dropna(subset=['content'])\n",
    "dfcpy = dfcpy.dropna(subset=['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plot the frequency of the top n words\n",
    "def plot_freq(fq, top_n):\n",
    "  common_words = fq.most_common(top_n)\n",
    "  # convert the list of tuples to a dictionary \n",
    "  all_freq = dict(common_words)\n",
    "  # create a plot\n",
    "  # plot most be less than 2^16 pixels in each direction\n",
    "  plt.figure(figsize = (top_n*0.1, 5))\n",
    "  plt.xticks(rotation = 90,fontsize = 5)\n",
    "  sns.lineplot(x = list(all_freq.keys()), y = list(all_freq.values()), color = 'red')\n",
    "  sns.barplot(x = list(all_freq.keys()), y = list(all_freq.values()))\n",
    "  plt.title(f'Top {top_n} most common words')\n",
    "  plt.xlabel('Words')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.grid(axis = 'y')\n",
    "  plt.show()\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcpy.content = dfcpy.content.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'<[\\w]+>|[\\w]+')\n",
    "dfcpy[\"tokenized\"] = dfcpy.content.apply(tokenizer.tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcpy.tokenized = dfcpy.tokenized.apply(rmv_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcpy.tokenized = dfcpy.tokenized.apply(stem_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocabulary(dfcpy.tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_domain_with_type(df):\n",
    "  top_domains = df.domain.value_counts().head(20).index\n",
    "  df = df[df.domain.isin(top_domains)]\n",
    "  df = df.groupby(['domain', 'type']).size().unstack().fillna(0)\n",
    "\n",
    "  df.plot(kind='bar', stacked=True, figsize=(10,5), title='Domain distribution with types')\n",
    "  plt.show()\n",
    "  return\n",
    "\n",
    "\n",
    "num_freq = vocab.get(\"<num>\",0)\n",
    "plot_freq(vocab, 100)\n",
    "\n",
    "# top 20 domains with their types\n",
    "plot_domain_with_type(dfcpy)\n",
    "\n",
    "# pie chart for the distribution of the types\n",
    "dfcpy.type.value_counts().plot.pie(autopct='%1.1f%%', figsize=(10,5), title='Types distribution')\n",
    "plt.show()\n",
    "\n",
    "# ammount of dropped rows\n",
    "print(f\"Number of dropped rows: {df.shape[0] - dfcpy.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcpy.tokenized = dfcpy.tokenized.apply(lambda x: ' '.join(x))\n",
    "dfcpy.to_csv('cleaned_news.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('cleaned_news.csv', usecols=['tokenized', 'type'], engine='c', dtype = str)\n",
    "dfcpy = df.copy()\n",
    "# label is 1 if the article is fake, 0 if the article is reliable\n",
    "dfcpy['label'] = dfcpy['type'].map({'fake': 1, 'conspiracy': 1, 'junksci': 1, 'bias': 1, 'clickbait': 0, 'political': 0, 'reliable': 0})\n",
    "dfcpy = dfcpy.dropna(subset=['label'])\n",
    "dfcpy['label'] = dfcpy['label'].astype(int)\n",
    "\n",
    "X = dfcpy['tokenized']\n",
    "y = dfcpy['label']\n",
    "X.dropna(inplace=True)\n",
    "y = y.loc[X.index]\n",
    "\n",
    "\n",
    "train_ratio = 0.80\n",
    "validation_ratio = 0.10\n",
    "test_ratio = 0.10\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 1 - train_ratio, random_state = 42) \n",
    "x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size= test_ratio / (test_ratio + validation_ratio), random_state = 42)\n",
    "\n",
    "print(\"Training Set:\")\n",
    "print(x_train.head())\n",
    "x_train = x_train.apply(lambda x: np.str_(x))\n",
    "x_val = x_val.apply(lambda x: np.str_(x))\n",
    "x_test = x_test.apply(lambda x: np.str_(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the percentage distribution of 'reliable' vs. 'fake' articles\n",
    "grouped_type = dfcpy['label'].value_counts()\n",
    "grouped_type = grouped_type / grouped_type.sum() * 100\n",
    "\n",
    "# make a bar plot with percentages on bars\n",
    "plt.bar([0, 1], grouped_type, tick_label=['Reliable', 'Fake'], color=['blue', 'red'])\n",
    "plt.text(0, grouped_type[0], f'{grouped_type[0]:.2f}%', ha='center', va='bottom')\n",
    "plt.text(1, grouped_type[1], f'{grouped_type[1]:.2f}%', ha='center', va='bottom')\n",
    "plt.xlabel('Article Type')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage Distribution of Reliable vs. Fake Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "import seaborn as sns\n",
    "def evaluate_model(y_val, y_pred):\n",
    "    # Confusion matrix\n",
    "    confusion_matrix = metrics.confusion_matrix(y_val, y_pred, labels=[1, 0])\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='g', cmap='Blues', xticklabels=['fake', 'real'], yticklabels=['fake', 'real'])\n",
    "\n",
    "    # Classification report\n",
    "    classification_report = metrics.classification_report(y_val, y_pred, target_names = ['fake', 'real'], zero_division=0)\n",
    "    print(classification_report)\n",
    "    accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "    f1 = metrics.f1_score(y_val, y_pred)\n",
    "    print(\"f1 score:\", f1)\n",
    "    print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: A simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# scaling the data\n",
    "model = Pipeline([(\"tfidf\", TfidfVectorizer(lowercase = False,max_features=1000, token_pattern=r'<[\\w]+>|[\\w]+')),\n",
    "                (\"scaler\", StandardScaler(with_mean=False)), \n",
    "                (\"log_reg\", LogisticRegression(max_iter=10000)) # increase max_iter to convergence \n",
    "                ])\n",
    "# sag på fulde dataset\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_val)\n",
    "evaluate_model(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Advanced model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 models: \n",
    "- LinearSVM\n",
    "- Naive bayes\n",
    "- Random forrest\n",
    "\n",
    "2 vector representations:\n",
    "- TF-IDF, 3 grams\n",
    "- Word embedding (word2vec)\n",
    "\n",
    "We perfrom cross validation on hyper paramaters to find the best hyperparameters for each model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(lowercase = False,max_features=1000, token_pattern=r'<[\\w]+>|[\\w]+' ,ngram_range=(2, 2))),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ('svd', TruncatedSVD(n_components=256)) # dimension reduction (kan udkommenteres først)\n",
    "    ])\n",
    "\n",
    "X_train_TFIDF = pipeline.fit_transform(x_train, y_train) \n",
    "X_val_TFIDF = pipeline.transform(x_val)\n",
    "X_test_TFIDF = pipeline.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "This 'Pipeline' has no attribute 'fit_transform'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/ex1/lib/python3.10/site-packages/sklearn/utils/_available_if.py:29\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor._check\u001b[0;34m(self, obj, owner)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 29\u001b[0m     check_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ex1/lib/python3.10/site-packages/sklearn/pipeline.py:481\u001b[0m, in \u001b[0;36mPipeline._can_fit_transform\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_can_fit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 481\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    483\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    484\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 23\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m weight_matrix\n\u001b[1;32m     18\u001b[0m pipeline_w2v \u001b[38;5;241m=\u001b[39m Pipeline([\n\u001b[1;32m     19\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtokenizer\u001b[39m\u001b[38;5;124m'\u001b[39m, Tokenizer()),\n\u001b[1;32m     20\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvectorizer\u001b[39m\u001b[38;5;124m'\u001b[39m, get_weight_matrix(w2v_model, tokenizer\u001b[38;5;241m.\u001b[39mword_index))\n\u001b[1;32m     21\u001b[0m ])\n\u001b[0;32m---> 23\u001b[0m X_train_W2V \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline_w2v\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m(x_train)\n\u001b[1;32m     24\u001b[0m X_val_W2V \u001b[38;5;241m=\u001b[39m pipeline_w2v\u001b[38;5;241m.\u001b[39mtransform(x_val)\n\u001b[1;32m     25\u001b[0m X_test_W2V \u001b[38;5;241m=\u001b[39m pipeline_w2v\u001b[38;5;241m.\u001b[39mtransform(x_test)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ex1/lib/python3.10/site-packages/sklearn/utils/_available_if.py:40\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__\u001b[0;34m(self, obj, owner)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__get__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj, owner\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;66;03m# delegate only on instances, not the classes.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;66;03m# this is to allow access to the docstrings.\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mowner\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m         out \u001b[38;5;241m=\u001b[39m MethodType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn, obj)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;66;03m# This makes it possible to use the decorated method as an unbound method,\u001b[39;00m\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m# for instance when monkeypatching.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/ex1/lib/python3.10/site-packages/sklearn/utils/_available_if.py:31\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor._check\u001b[0;34m(self, obj, owner)\u001b[0m\n\u001b[1;32m     29\u001b[0m     check_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck(obj)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr_err_msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m check_result:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr_err_msg)\n",
      "\u001b[0;31mAttributeError\u001b[0m: This 'Pipeline' has no attribute 'fit_transform'"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(sentences=x_train, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "def get_weight_matrix(model, vocab):\n",
    "    vocab_size = len(vocab) + 1\n",
    "    weight_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in vocab.items():\n",
    "        if word in model.wv:\n",
    "            weight_matrix[i] = model.wv.get_vector(word)\n",
    "    return weight_matrix\n",
    "\n",
    "pipeline_w2v = Pipeline([\n",
    "    ('tokenizer', Tokenizer()),\n",
    "    ('vectorizer', get_weight_matrix(w2v_model, tokenizer.word_index))\n",
    "])\n",
    "\n",
    "X_train_W2V = pipeline_w2v.fit_transform(x_train)\n",
    "X_val_W2V = pipeline_w2v.transform(x_val)\n",
    "X_test_W2V = pipeline_w2v.transform(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Linear SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def svc_advanced(x_train, y_train, x_val):\n",
    "    svc = LinearSVC(dual=True, max_iter=10000, random_state=42) \n",
    "    parameters = dict(C = [0.1 ,1, 10], loss = ['hinge', 'squared_hinge'], tol = [1e-3, 1, 10])\n",
    "\n",
    "    # Cross-validation\n",
    "    grid_search = GridSearchCV(svc, parameters, cv=3, n_jobs=-1, scoring = 'f1')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "    \n",
    "    return grid_search.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc_advanced(X_train_TFIDF, y_train, X_val_TFIDF)\n",
    "evaluate_model(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svc_advanced(X_train_W2V, y_train, X_val_W2V_)\n",
    "evaluate_model(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec on subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc2vec on subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "parameters = {\n",
    "    'nb__alpha': [0.01, 0.1, 1, 10, 100],  \n",
    "}\n",
    "\n",
    "pipeline_nb = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(ngram_range=(2, 2), lowercase = False, token_pattern=r'<[\\w]+>|[\\w]+')),\n",
    "    ('svd', TruncatedSVD(n_components=256)),\n",
    "    ('nb', nb)\n",
    "])\n",
    "\n",
    "# Cross-validation\n",
    "grid_search = GridSearchCV(pipeline_nb, parameters, cv=2, n_jobs=-1, scoring='f1')\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "y_pred = grid_search.predict(x_val)\n",
    "\n",
    "evaluate_model(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def naive_bayes_advanced(x_train, y_train, x_val):\n",
    "    nb = MultinomialNB()\n",
    "    parameters = dict(alpha = [0.01, 0.1, 1, 10, 100])\n",
    "\n",
    "\n",
    "    # Cross-validation\n",
    "    grid_search = GridSearchCV(nb, parameters, cv=2, n_jobs=-1, scoring='f1')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "\n",
    "    return grid_search.predict(x_val)\n",
    "\n",
    "y_pred = naive_bayes_advanced(X_train_TFIDF, y_train, X_val_TFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = naive_bayes_advanced(X_train_TFIDF, y_train, X_val_TFIDF)\n",
    "evaluate_model(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = naive_bayes_advanced(X_train_W2V, y_train, X_val_W2V)\n",
    "# evaluate_model(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Random forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def random_forest_advanced(x_train, y_train, x_val):\n",
    "    rf = RandomForestClassifier()\n",
    "    parameters = dict(n_estimators = [100, 200, 300], \n",
    "                      max_depth = [10, 20, 30])\n",
    "\n",
    "    # Cross-validation\n",
    "    grid_search = GridSearchCV(rf, parameters, cv=2, n_jobs=-1, scoring='f1')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Parameters:\", best_params)\n",
    "\n",
    "    return grid_search.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# pipeline_rf = Pipeline([\n",
    "#     ('vectorizer', TfidfVectorizer(ngram_range=(1, 2))),\n",
    "#     ('rf', rf)from sklearn.decomposition import TruncatedSVD\n",
    "# ])\n",
    "\n",
    "\n",
    "# param_grid = {\n",
    "#     'rf__n_estimators': [100, 200, 300],  \n",
    "#     'rf__max_depth': [10, 20, 30],  \n",
    "# }\n",
    "\n",
    "# # Cross-validation\n",
    "# grid_search = GridSearchCV(pipeline_rf, param_grid, cv=5, n_jobs=-1, scoring='f1')\n",
    "# grid_search.fit(x_train, y_train)\n",
    "\n",
    "# best_params = grid_search.best_params_\n",
    "# print(\"Best Parameters:\", best_params)\n",
    "\n",
    "# y_pred = grid_search.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = random_forest_advanced(X_train_TFIDF, y_train, X_val_TFIDF)\n",
    "evaluate_model(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = random_forest_advanced(X_train_W2V, y_train, X_val_W2V)\n",
    "# evaluate_model(y_val, y_pred)\n",
    "# use word embeddings on the training data\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(sentences=x_train, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "# Vocab size\n",
    "vocab_size = len(w2v_model.wv.key_to_index)\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "#Present words as numbers\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "\n",
    "x_train_numbers = tokenizer.texts_to_sequences(x_train)\n",
    "\n",
    "#numerical replesentation of a few words\n",
    "word_index = tokenizer.word_index\n",
    "for word, num in word_index.items():\n",
    "    print(f\"{word} -> {num}\")\n",
    "    if num == 10:\n",
    "        break  \n",
    "\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Function to create weight matrix from word2vec gensim model\n",
    "def get_weight_matrix(model, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, 100))\n",
    "    # step vocab, store vectors using the Tokenizer's int mapping\n",
    "    for word, i in vocab.items():\n",
    "        if word in model.wv:\n",
    "            weight_matrix[i] = model.wv.get_vector(word)\n",
    "    return weight_matrix\n",
    "\n",
    "embedding_vectors = get_weight_matrix(w2v_model, word_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_enviroment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
