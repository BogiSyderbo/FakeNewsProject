{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Project\n",
    "The goal of this project is to create a fake news prediction system. Fake news is a major problem that can have serious negative effects on how people understand the world around them. You will work with a dataset containing real and fake news in order to train a simple and a more advanced classifier to solve this problem. This project covers the full Data Science pipeline, from data processing, to modelling, to visualization and interpretation.\n",
    "\n",
    "We ran the notebook with the following specs:\n",
    "- CPU: Intel(R) Xeon(R) CPU E5-2687W v3 @ 3.10GHz\n",
    "- Cores: 10\n",
    "- Threads: 20\n",
    "- Memory: 64GB Ram\n",
    "# Part 1 Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is used to process The fake news corpus. Since content will be used for our models we drop any rows that don't have any content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd \n",
    "\n",
    "notebook_start_time = time()\n",
    "df = pd.read_csv(\"news_sample.csv\")\n",
    "dfcpy = df.copy()\n",
    "dfcpy = dfcpy.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've implemented data processing functions to do the following:\n",
    "- Clean the text\n",
    "- Tokenize the text\n",
    "- Remove stopwords\n",
    "- Remove word variations with stemming\n",
    "\n",
    "We use nltk because it has built-in support for many of these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from cleantext import clean\n",
    "\n",
    "def clean_text(text):\n",
    "  clean_text = re.sub(r'([A-Z][A-z]+.?) ([0-9]{1,2}?), ([0-9]{4})', '<DATE>', text)\n",
    "  clean_text = clean(clean_text,\n",
    "    lower=True,\n",
    "    no_urls=True, replace_with_url=\"<URL>\",\n",
    "    no_emails=True, replace_with_email=\"<EMAIL>\",\n",
    "    no_numbers=True, replace_with_number= r\"<NUM>\",\n",
    "    no_currency_symbols=True, replace_with_currency_symbol=\"<CUR>\",\n",
    "    no_punct=True, replace_with_punct=\"\",\n",
    "    no_line_breaks=True \n",
    "  )\n",
    "  return clean_text\n",
    "\n",
    "def rmv_stopwords(tokens):\n",
    "  stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "  tokens = [word for word in tokens if word not in stop_words]\n",
    "  return tokens\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "  stemmer=PorterStemmer()\n",
    "  Output=[stemmer.stem(word) for word in tokens]\n",
    "  return Output\n",
    "\n",
    "def build_vocabulary(df_tokens):\n",
    "  tokens = []\n",
    "  for lst in df_tokens:\n",
    "    tokens += lst\n",
    "  token_counter = Counter(tokens)\n",
    "  return token_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning:\n",
      "vocabulary size: 16577\n",
      "\n",
      "After removing stopwords:\n",
      "vocabulary size: 16445\n",
      "reduction rate of the vocabulary size: 0.80%\n",
      "\n",
      "After stemming:\n",
      "vocabulary size: 11031\n",
      "reduction rate of the vocabulary size: 32.92%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfcpy = df.copy()\n",
    "\n",
    "dfcpy.content = dfcpy.content.apply(clean_text)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'<[\\w]+>|[\\w]+')\n",
    "dfcpy[\"tokenized\"] = dfcpy.content.apply(tokenizer.tokenize)\n",
    "\n",
    "vocab = build_vocabulary(dfcpy.tokenized)\n",
    "vocab_size = len(vocab)\n",
    "print(\"After cleaning:\")\n",
    "print(f\"vocabulary size: {vocab_size}\\n\")\n",
    "\n",
    "dfcpy.tokenized = dfcpy.tokenized.apply(rmv_stopwords)\n",
    "vocab = build_vocabulary(dfcpy.tokenized)\n",
    "# reduction rate of the vocabulary size\n",
    "reduction = ((vocab_size - len(vocab))/vocab_size)*100\n",
    "vocab_size = len(vocab)\n",
    "print(\"After removing stopwords:\")\n",
    "print(f\"vocabulary size: {vocab_size}\")\n",
    "print(f\"reduction rate of the vocabulary size: {reduction:.2f}%\\n\")\n",
    "\n",
    "dfcpy.tokenized = dfcpy.tokenized.apply(stem_tokens)\n",
    "vocab = build_vocabulary(dfcpy.tokenized)\n",
    "reduction = ((vocab_size - len(vocab))/vocab_size)*100\n",
    "vocab_size = len(vocab)\n",
    "print(\"After stemming:\")\n",
    "print(f\"vocabulary size: {vocab_size}\")\n",
    "print(f\"reduction rate of the vocabulary size: {reduction:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply our data processing pipeline from task 1 on the *995k FakeNewsCorpus*. \n",
    "\n",
    "Pandas is slow when used on bigger amounts of data, this is because it dosen't allow for multithreading. Modin and ray are libaries that optimize pandas by allowing pandas to run on all cores, thereby giving a speed up for the data processing. By using modin with ray as an engine you can use pandas as usual, but have it use all threads in the CPU. We used a Intel Xeon cpu with 20 threads and therefore saw huge performance gain by using modin. \n",
    "\n",
    "Modin and ray can be installed by running the following command:\n",
    "pip install \"modin[ray]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.config as modin_cfg\n",
    "modin_cfg.Engine.put(\"ray\") # make sure to use Ray engine and other than could be installed\n",
    "import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only read the columns we need\n",
    "df = pd.read_csv(\"995,000_rows.csv\", \n",
    "                 usecols=['content', 'type', 'title', 'domain'], \n",
    "                 engine='c', \n",
    "                 dtype = str)\n",
    "dfcpy = df.copy()\n",
    "dfcpy = dfcpy.dropna(subset=['content'])\n",
    "dfcpy = dfcpy.dropna(subset=['type'])\n",
    "dfcpy = dfcpy.dropna(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "dfcpy.title = dfcpy.title.apply(clean_text)\n",
    "dfcpy.content = dfcpy.content.apply(clean_text)\n",
    "print(f\"time to clean the data: {time() - start:.2f} sec\")\n",
    "\n",
    "t = time()\n",
    "tokenizer = RegexpTokenizer(r'<[\\w]+>|[\\w]+')\n",
    "dfcpy.title = dfcpy.title.apply(tokenizer.tokenize)\n",
    "dfcpy.combined = dfcpy.combined.apply(tokenizer.tokenize)\n",
    "print(f\"time to tokenize the data: {(time() - t)/60:.2f} min\" )\n",
    "\n",
    "t = time()\n",
    "dfcpy.title = dfcpy.title.apply(rmv_stopwords)\n",
    "dfcpy.content = dfcpy.content.apply(rmv_stopwords)\n",
    "print(f\"time to remove stopwords: {(time() - t)/60:.2f} min\")\n",
    "\n",
    "t = time()\n",
    "dfcpy.title = dfcpy.title.apply(stem_tokens)\n",
    "dfcpy.content = dfcpy.content.apply(stem_tokens)\n",
    "print(f\"time to stem the data: {(time() - t)/60:..2f} sec\")\n",
    "\n",
    "print(f\"total time: {(time() - start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've explored the dataset and made some observations which are used to determine importance of certain metadata in the fake news corpus, such observations are:\n",
    "\n",
    "- The amount of numerics in the dataset\n",
    "- The 100 most frequent words\n",
    "- The 20 most frequent domains and how their articles are classified in terms of type\n",
    "- The distrubtion of types in the dataset\n",
    "- The amount of rows missing content, title or type (amount of rows that will be dropped from the dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "vocab_content = build_vocabulary(dfcpy.content)\n",
    "print(f\"time to build vocabulary for content: {(time() - start)/60:.2f} min\")\n",
    "\n",
    "start = time()\n",
    "vocab_title = build_vocabulary(dfcpy.title)\n",
    "print(f\"time to build vocabulary for title: {(time() - start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plot the frequency of the top n words\n",
    "def plot_freq(counter, top_n):\n",
    "  common_words = counter.most_common(top_n)\n",
    "\n",
    "  all_freq = {}\n",
    "  for word, freq in common_words:\n",
    "    all_freq[word] = freq\n",
    "\n",
    "  plt.figure(figsize = (top_n*0.1, 5))\n",
    "  plt.xticks(rotation = 90,fontsize = 5)\n",
    "  sns.lineplot(x = list(all_freq.keys()), y = list(all_freq.values()), color = 'red')\n",
    "  sns.barplot(x = list(all_freq.keys()), y = list(all_freq.values()))\n",
    "  plt.title(f'Top {top_n} most common words')\n",
    "  plt.xlabel('Words')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.grid(axis = 'y')\n",
    "  plt.show()\n",
    "  return\n",
    "\n",
    "def plot_domain_with_type(df):\n",
    "  top_domains = df.domain.value_counts().head(20).index\n",
    "  df = df[df.domain.isin(top_domains)]\n",
    "  df = df.groupby(['domain', 'type']).size().unstack().fillna(0)\n",
    "\n",
    "  df.plot(kind='bar', stacked=True, figsize=(10,5), title='Domain distribution with types')\n",
    "  plt.show()\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 100 most frequent words\n",
    "print(\"numerics in content: \", vocab_content[\"<num>\"])\n",
    "plot_freq(vocab_content, 100)\n",
    "print(\"numerics in titles: \", vocab_title[\"<num>\"])\n",
    "plot_freq(vocab_title, 100)\n",
    "\n",
    "# top 20 domains with their types\n",
    "plot_domain_with_type(dfcpy)\n",
    "\n",
    "# pie chart for the distribution of the types\n",
    "dfcpy.type.value_counts().plot.pie(autopct='%1.1f%%', figsize=(10,5), title='Types distribution')\n",
    "plt.show()\n",
    "\n",
    "# ammount of dropped rows\n",
    "print(f\"Number of dropped rows: {df.shape[0] - dfcpy.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exporting the cleaned dataset we have to make sure the tokens are stored correctly in the csv. A csv can correctly store a python list, therefore we store the tokens as a string using space as a seperator for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcpy.content = dfcpy.content.apply(lambda x: ' '.join(x))\n",
    "dfcpy.title = dfcpy.title.apply(lambda x: ' '.join(x))\n",
    "dfcpy.to_csv('cleaned_news.csv', index=False)\n",
    "print(\"done cleaning the data\")\n",
    "\n",
    "# shutdown the ray engine to free up the memory\n",
    "import ray \n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the types we label articles as either fake or reliable. Some article types are omitted since it's ambigious wheter they are fake news or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('cleaned_news.csv', usecols=['content', 'type', 'title'], engine='c', dtype = str)\n",
    "dfcpy = df.copy()\n",
    "\n",
    "# label is 1 if the article is fake, 0 if the article is reliable\n",
    "dfcpy['label'] = dfcpy['type'].map({'fake': 1, \n",
    "                                    'conspiracy': 1, \n",
    "                                    'junksci': 1, \n",
    "                                    'bias': 1, \n",
    "                                    'clickbait': 0, \n",
    "                                    'political': 0, \n",
    "                                    'reliable': 0})\n",
    "dfcpy = dfcpy.dropna(subset=['label'])\n",
    "dfcpy['label'] = dfcpy['label'].astype(int)\n",
    "\n",
    "dfcpy = dfcpy.dropna(subset=['content'])\n",
    "dfcpy = dfcpy.dropna(subset=['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into a random 80/10/10 split where 80% is used for training. 10% is used for validation and 10% is used for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Splitting the data into training (80%) and the rest (20%)\n",
    "train_df, rest_df = train_test_split(dfcpy, test_size=0.2, random_state=42, stratify=dfcpy['label'])\n",
    "# Splitting the rest into validation (50%) and test (50%)\n",
    "validation_df, test_df = train_test_split(rest_df, t\n",
    "                                          est_size=0.5, \n",
    "                                          random_state=42, \n",
    "                                          stratify=rest_df['label'])\n",
    "\n",
    "content_train, title_train ,y_train = train_df['content'], train_df['title'], train_df['label']\n",
    "content_val, title_val, y_val = validation_df['content'], validation_df['title'], validation_df['label']\n",
    "content_test, title_test, y_test = test_df['content'], test_df['title'], test_df['label']\n",
    "\n",
    "print(\"Training Set:\")\n",
    "print(train_df.content.head())\n",
    "print(train_df.title.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the Distrubution of Fake and reliable articles to get and idea on wheter our data is balanced or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the percentage distribution of 'reliable' vs. 'fake' articles\n",
    "grouped_type = dfcpy['label'].value_counts()\n",
    "grouped_type = grouped_type / grouped_type.sum() * 100\n",
    "\n",
    "# make a bar plot with percentages on bars\n",
    "plt.bar([0, 1], grouped_type, tick_label=['Reliable', 'Fake'], color=['blue', 'red'])\n",
    "plt.text(0, grouped_type[0], f'{grouped_type[0]:.2f}%', ha='center', va='bottom')\n",
    "plt.text(1, grouped_type[1], f'{grouped_type[1]:.2f}%', ha='center', va='bottom')\n",
    "plt.xlabel('Article Type')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage Distribution of Reliable vs. Fake Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and cleaning extra reliable articles scraped from BBC news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra = pd.read_csv(\"scraped_articles.csv\", usecols=['content'])\n",
    "df_extra_cpy = df_extra.copy()\n",
    "df_extra_cpy = df_extra_cpy.dropna(subset=['content'])\n",
    "df_extra_cpy.content = df_extra_cpy.content.apply(clean_text)\n",
    "tokenizer = RegexpTokenizer(r'<[\\w]+>|[\\w]+')\n",
    "df_extra_cpy.content = df_extra_cpy.content.apply(tokenizer.tokenize)\n",
    "df_extra_cpy.content = df_extra_cpy.content.apply(rmv_stopwords)\n",
    "df_extra_cpy.content = df_extra_cpy.content.apply(stem_tokens)\n",
    "df_extra_cpy['label'] = 0\n",
    "\n",
    "\n",
    "df_extra_cpy.content = df_extra_cpy.content.apply(lambda x: ' '.join(x))\n",
    "x_train_extra = pd.concat([content_train, df_extra_cpy.content], ignore_index=True)\n",
    "y_train_extra = pd.concat([y_train, df_extra_cpy.label], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "def make_confusion_matrix(y_val, y_pred,model_name):\n",
    "    confusion_matrix = metrics.confusion_matrix(y_val, y_pred, labels=[1, 0])\n",
    "    sns.heatmap(confusion_matrix, \n",
    "                annot=True, \n",
    "                fmt='g', \n",
    "                cmap='Blues', \n",
    "                xticklabels=['real', 'fake'], \n",
    "                yticklabels=['real', 'fake'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'{model_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: A simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use logistic regression for our simple model. The model is simple in terms of vector representation (bag of words) and lack of hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.metrics as metrics\n",
    "from scipy.sparse import hstack\n",
    "from joblib import dump\n",
    "\n",
    "vectorrizer = CountVectorizer(lowercase = False, max_features = 7000, token_pattern=r'<[\\w]+>|[\\w]+')\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorrizer), \n",
    "    ('scaler', StandardScaler(with_mean=False))\n",
    "    ])\n",
    "\n",
    "model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "\n",
    "# making bag of words for the content and extra data\n",
    "BoW_extra = pipeline.fit_transform(x_train_extra)\n",
    "BoW_content_val = pipeline.transform(content_val)\n",
    "\n",
    "# Model with only content, but with extra data\n",
    "model.fit(BoW_extra, y_train_extra)\n",
    "y_pred = model.predict(BoW_content_val)\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"Only content, but with extra data:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "# mkain bag of words for the content\n",
    "BoW_content_train = pipeline.fit_transform(content_train)\n",
    "BoW_content_val = pipeline.transform(content_val)\n",
    "content_test_bow = pipeline.transform(content_test)\n",
    "\n",
    "# Model with only content\n",
    "model.fit(BoW_content_train, y_train)\n",
    "y_pred = model.predict(BoW_content_val)\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"Only content:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy:\", accuracy)\n",
    "# saving the model\n",
    "dump(model, 'models/simple_model_content.joblib')\n",
    "\n",
    "# making bag of words for the title and content\n",
    "BoW_title_train = pipeline.fit_transform(title_train)\n",
    "BoW_title_val = pipeline.transform(title_val)\n",
    "BoW_combined_train = hstack((BoW_content_train, BoW_title_train))\n",
    "BoW_combined_val = hstack((BoW_content_val, BoW_title_val))\n",
    "\n",
    "# Model with content and title\n",
    "model.fit(BoW_combined_train, y_train)\n",
    "y_pred = model.predict(BoW_combined_val)\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"\\nContent and title:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy:\", accuracy)\n",
    "# saving the model\n",
    "dump(model, 'models/simple_model_combined.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Advanced model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tried 3 models: \n",
    "- LinearSVM\n",
    "- Naive bayes\n",
    "- Logistic regression (using TF-IDF and cross validation)\n",
    "\n",
    "We tried 2 vector representations:\n",
    "- TF-IDF (1, 2 and 3 grams )\n",
    "- Word embedding (word2vec)\n",
    "\n",
    "We perfrom cross validation (gridsearch) on hyper paramaters to find the best hyperparameters for each model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Linear SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from joblib import dump\n",
    "\n",
    "def svm(x_train, y_train, x_val, model_name):\n",
    "    svc = LinearSVC(max_iter=10000, dual=False, random_state=42) \n",
    "    parameters = dict(C=[0.001, 0.1, 1, 10], loss = ['hinge', 'squared_hinge'])\n",
    "    # Cross-validation\n",
    "    grid_search = GridSearchCV(svc, parameters, cv=3, n_jobs=-1, scoring = 'f1', pre_dispatch=3)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Parameters for svm:\", best_params)\n",
    "\n",
    "    dump(grid_search, f'models/{model_name}.joblib')\n",
    "    \n",
    "    return grid_search.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from joblib import dump\n",
    "\n",
    "def naive_bayes(x_train, y_train, x_val, model_name):\n",
    "    nb = MultinomialNB(random_state=42)\n",
    "    parameters = dict(alpha=[0.01,0.1, 1, 10])\n",
    "    # Cross-validation\n",
    "    grid_search = GridSearchCV(nb, parameters, cv=3, n_jobs=-1, scoring = 'f1')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best parameters for Naive Bayes model:\", best_params)\n",
    "\n",
    "    dump(grid_search, f'models/{model_name}.joblib')\n",
    "    \n",
    "    return grid_search.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Logistic regression\n",
    "We noticed our simple model performed quite well, therefore we tried to optimize hyperparameters and use n-grams to see if this would improve the simple model further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from joblib import dump\n",
    "from time import time\n",
    "\n",
    "def logistic_advanced(x_train, y_train, x_val, model_name):\n",
    "    time_start = time()\n",
    "    logistic = LogisticRegression(max_iter = 10000, random_state=42)\n",
    "    parameters = dict(C=[0.001, 0.1, 1, 10], solver=['sag','saga'])\n",
    "    # Cross-validation\n",
    "    grid_search = GridSearchCV(logistic, parameters, cv=3, n_jobs=-1, scoring = 'f1', pre_dispatch=3)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    print(f\"time to train the model: {time() - time_start:.2f} sec\")\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best parameters for logistic regression model:\", best_params)\n",
    "    # saving the model\n",
    "    dump(grid_search, f'models/{model_name}.joblib')\n",
    "    \n",
    "    return grid_search.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import hstack\n",
    "from time import time\n",
    "\n",
    "def make_TFIDF(features, ngrams, metadata):\n",
    "    time_start = time()\n",
    "    global content_test, content_train, content_val, title_test, title_train, title_val\n",
    "    global y_test, y_train, y_val\n",
    "    pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(lowercase = False, \n",
    "                                   max_features=features, \n",
    "                                   min_df = 1, \n",
    "                                   max_df= 0.9, \n",
    "                                   token_pattern=r'<[\\w]+>|[\\w]+',\n",
    "                                   ngram_range =  ngrams)),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ])\n",
    "    \n",
    "    if metadata == \"content\":\n",
    "        content_train_TFIDF = pipeline.fit_transform(content_train, y_train) \n",
    "        content_val_TFIDF = pipeline.transform(content_val)\n",
    "        content_test_TFIDF = pipeline.transform(content_test)\n",
    "        print(f\"time to make TFIDF for {metadata}: {time() - time_start:.2f} sec\")\n",
    "        return content_train_TFIDF, content_val_TFIDF, content_test_TFIDF\n",
    "    if metadata == \"title\":\n",
    "        title_train_TFIDF = pipeline.fit_transform(title_train, y_train)\n",
    "        title_val_TFIDF = pipeline.transform(title_val)\n",
    "        title_test_TFIDF = pipeline.transform(title_test)\n",
    "        print(f\"time to make TFIDF for {metadata}: {time() - time_start:.2f} sec\")\n",
    "        return title_train_TFIDF, title_val_TFIDF, title_test_TFIDF\n",
    "    if metadata == \"combined\":\n",
    "        content_train_TFIDF = pipeline.fit_transform(content_train, y_train) \n",
    "        content_val_TFIDF = pipeline.transform(content_val)\n",
    "        content_test_TFIDF = pipeline.transform(content_test)\n",
    "\n",
    "        title_train_TFIDF = pipeline.fit_transform(title_train, y_train)\n",
    "        title_val_TFIDF = pipeline.transform(title_val)\n",
    "        title_test_TFIDF = pipeline.transform(title_test)\n",
    "        \n",
    "        X_train_TFIDF = hstack((content_train_TFIDF, title_train_TFIDF))\n",
    "        X_val_TFIDF = hstack((content_val_TFIDF, title_val_TFIDF))\n",
    "        X_test_TFIDF = hstack((content_test_TFIDF, title_test_TFIDF))\n",
    "        print(f\"time to make TFIDF for {metadata}: {time() - time_start:.2f} sec\")\n",
    "        return X_train_TFIDF, X_val_TFIDF, X_test_TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_TFIDF, X_val_TFIDF, X_test_TFIDF = make_TFIDF(7000, (1, 1), \"content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm(X_train_TFIDF, y_train, X_val_TFIDF, 'svm_1gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"Support vector machine:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic regression:\")\n",
    "y_pred = logistic_advanced(X_train_TFIDF, y_train, X_val_TFIDF, 'logistic_1gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_TFIDF, X_val_TFIDF, X_test_TFIDF = make_TFIDF(None, (1, 1))\n",
    "print(\"Naive Bayes:\")\n",
    "y_pred = naive_bayes(X_train_TFIDF, y_train, X_val_TFIDF, 'naive_bayes_1gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_TFIDF, X_val_TFIDF, X_test_TFIDF = make_TFIDF(7000, (2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm(X_train_TFIDF, y_train, X_val_TFIDF, 'svm_2gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"Support vector machine:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic regression:\")\n",
    "y_pred = logistic_advanced(X_train_TFIDF, y_train, X_val_TFIDF, 'logistic_2gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_TFIDF, X_val_TFIDF, X_test_TFIDF = make_TFIDF(None, (2, 2))\n",
    "print(\"Naive Bayes:\")\n",
    "y_pred = naive_bayes(X_train_TFIDF, y_train, X_val_TFIDF, 'naive_bayes_2gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 grams: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_TFIDF, X_val_TFIDF, X_test_TFIDF = make_TFIDF(7000, (3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm(X_train_TFIDF, y_train, X_val_TFIDF, 'svm_2gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"Support vector machine:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic regression:\")\n",
    "y_pred = logistic_advanced(X_train_TFIDF, y_train, X_val_TFIDF, 'logistic_2gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_TFIDF, X_val_TFIDF, X_test_TFIDF = make_TFIDF(None, (3, 3))\n",
    "print(\"Naive Bayes:\")\n",
    "y_pred = naive_bayes(X_train_TFIDF, y_train, X_val_TFIDF, 'naive_bayes_2gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Doc2Vec model on the full dataset takes a long time and is actually not needed, it's sucfficient to train the model on a subset where we take n samples from each type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new split using x amount of each type of article\n",
    "type_amount = 1000\n",
    "\n",
    "dfcpy_subset = dfcpy.groupby('type').head(type_amount)\n",
    "print(\"Number of articles of each type in the new dataset:\"\n",
    "        ,dfcpy['type'].value_counts())\n",
    "\n",
    "dfcpy_subset = dfcpy_subset.dropna(subset=['content'])\n",
    "dfcpy_subset = dfcpy_subset.dropna(subset=['title'])\n",
    "\n",
    "X = dfcpy_subset.content \n",
    "y = dfcpy_subset.label\n",
    "\n",
    "print(\"Training Set:\")\n",
    "print(train_df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def doc2vec(X, y, size, win, epo):\n",
    "    time_start = time()\n",
    "    doc2vec_model = Doc2Vec(vector_size=size, window=win, min_count=1, epochs = epo, workers = 19)\n",
    "    tagged_data = [TaggedDocument(words = word_tokenize(doc), tags=[i]) for i, doc in enumerate(X)]\n",
    "\n",
    "    doc2vec_model.build_vocab(tagged_data)  \n",
    "    doc2vec_model.train(tagged_data, \n",
    "                        total_examples = doc2vec_model.corpus_count, \n",
    "                        epochs = doc2vec_model.epochs)\n",
    "    doc_vectors = [doc2vec_model.infer_vector(word_tokenize(doc)) for doc in X]\n",
    "\n",
    "    # scale the data\n",
    "    scaler = StandardScaler()\n",
    "    doc_vectors = scaler.fit_transform(doc_vectors)\n",
    "    X_train_D2V, X_rest_D2V, y_train_D2V, y_res_D2V = train_test_split(doc_vectors,y, \n",
    "                                                                       test_size=0.2, \n",
    "                                                                       random_state=42)\n",
    "    \n",
    "    X_val_D2V, X_test_D2V, y_val_D2V, y_test_D2V = train_test_split(X_rest_D2V, \n",
    "                                                                    y_res_D2V, \n",
    "                                                                    test_size=0.5, \n",
    "                                                                    random_state=42)\n",
    "    print(f\"time to train the model: {time() - time_start:.2f} sec\")\n",
    "    return X_train_D2V, X_val_D2V, X_test_D2V, y_train_D2V, y_val_D2V, y_test_D2V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_D2V, X_val_D2V, X_test_D2V, y_train_D2V, y_val_D2V, y_test_D2V = doc2vec(X, y, 100, 5, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm(X_train_D2V, y_train_D2V, X_val_D2V, 'svm_D2V')\n",
    "accuracy = metrics.accuracy_score(y_val_D2V, y_pred)\n",
    "f1 = metrics.f1_score(y_val_D2V, y_pred)\n",
    "print(\"Support vector machine:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)\n",
    "make_confusion_matrix(y_val_D2V, y_pred, \"SVM with Doc2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logistic_advanced(X_train_D2V, y_train_D2V, X_val_D2V, 'logistic_D2V')\n",
    "f1 = metrics.f1_score(y_val_D2V, y_pred)\n",
    "accuracy = metrics.accuracy_score(y_val_D2V, y_pred)\n",
    "print(\"Logistic regression:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)\n",
    "make_confusion_matrix(y_val_D2V, y_pred, \"Logistic Regression with Doc2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = naive_bayes(X_train_D2V, y_train_D2V, X_val_D2V, 'naive_bayes_D2V')\n",
    "f1 = metrics.f1_score(y_val_D2V, y_pred)\n",
    "accuracy = metrics.accuracy_score(y_val_D2V, y_pred)\n",
    "print(\"Naive Bayes:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)\n",
    "make_confusion_matrix(y_val_D2V, y_pred, \"Naive Bayes with Doc2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is with tuned hyperparameters is slightly better than linearsvc however it takes double the amount of time to train the logistic regression model, therefore we have chosen the Support vector machine instead as our model to test and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "liar_train = pd.read_csv('train.tsv', sep='\\t', header=None)\n",
    "liar_val = pd.read_csv('valid.tsv', sep='\\t', header=None)\n",
    "liar_test = pd.read_csv('test.tsv', sep='\\t', header=None)\n",
    "liar = pd.concat([liar_train, liar_val, liar_test], ignore_index=True)\n",
    "liar_cpy = liar.copy()\n",
    "\n",
    "liar_cpy[2] = liar_cpy[2].apply(clean_text)\n",
    "tokenizer = RegexpTokenizer(r'<[\\w]+>|[\\w]+')\n",
    "liar_cpy[2] = liar_cpy[2].apply(tokenizer.tokenize)\n",
    "liar_cpy[2] = liar_cpy[2].apply(rmv_stopwords)\n",
    "liar_cpy[2] = liar_cpy[2].apply(stem_tokens)\n",
    "liar_cpy[2] = liar_cpy[2].apply(lambda x: ' '.join(x))\n",
    "\n",
    "labels_used = ['pants-fire', 'false', 'mostly-true', 'true']\n",
    "liar_cpy = liar_cpy.dropna(subset=[1])\n",
    "liar_cpy = liar_cpy[liar_cpy[1].isin(labels_used)]\n",
    "liar_cpy[1] = liar_cpy[1].map({'pants-fire': 1, \n",
    "                                         'false': 1, \n",
    "                                         'mostly-true': 0, \n",
    "                                         'true': 0})\n",
    "liar_cpy = liar_cpy.dropna(subset=[2])\n",
    "\n",
    "x_train_liar, x_test_liar, y_train_liar, y_test_liar = train_test_split(liar_cpy[2], \n",
    "                                                                        liar_cpy[1], \n",
    "                                                                        test_size=0.1, \n",
    "                                                                        random_state=42)\n",
    "\n",
    "pipeline_bow = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(max_features=7000, token_pattern=r'<[\\w]+>|[\\w]+')), \n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ])\n",
    "# 3500 hvis titler er med\n",
    "pipeline_tfidf = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(lowercase = False, \n",
    "                                   max_features=7000, \n",
    "                                   min_df = 1, \n",
    "                                   max_df= 0.9, \n",
    "                                   token_pattern=r'<[\\w]+>|[\\w]+',\n",
    "                                   ngram_range =  (1, 1))),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ])\n",
    "\n",
    "simple_model = load('models/simple_model_combined.joblib')\n",
    "advanced_model = load('models/svm_1gram_combined.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing models with FakeNewsCorpus test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_train_bow = pipeline_bow.fit_transform(content_train, y_train)\n",
    "content_val_bow = pipeline_bow.transform(content_val)\n",
    "content_test_bow = pipeline_bow.transform(content_test)\n",
    "simple_pred_test = simple_model.predict(content_test_bow)\n",
    "\n",
    "accuracy_simple = metrics.accuracy_score(y_test, simple_pred_test)\n",
    "f1_simple = metrics.f1_score(y_test, simple_pred_test)\n",
    "\n",
    "print(\"\\nSimple model:\")\n",
    "print(\"Test set:\")\n",
    "print(\"f1 score:\", f1_simple)\n",
    "print(\"accuracy score:\", accuracy_simple)\n",
    "make_confusion_matrix(y_test, simple_pred_test, \"Simple model\")\n",
    "\n",
    "content_train_tfidf = pipeline_tfidf.fit_transform(content_train, y_train)\n",
    "content_val_tfidf = pipeline_tfidf.transform(content_val)\n",
    "content_test_tfidf = pipeline_tfidf.transform(content_test)\n",
    "advanced_pred_test = advanced_model.predict(content_test_tfidf)\n",
    "\n",
    "accuracy_advanced = metrics.accuracy_score(y_test, advanced_pred_test)\n",
    "f1_advanced = metrics.f1_score(y_test, advanced_pred_test)\n",
    "\n",
    "print(\"\\nAdvanced model:\")\n",
    "print(\"Test set:\")\n",
    "print(\"f1 score:\", f1_advanced)\n",
    "print(\"accuracy score:\", accuracy_advanced)\n",
    "make_confusion_matrix(y_test, advanced_pred_test, \"Advanced model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content + titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_train_bow = pipeline_bow.fit_transform(content_train, y_train)\n",
    "content_val_bow = pipeline_bow.transform(content_val)\n",
    "content_test_bow = pipeline_bow.transform(content_test)\n",
    "title_test_bow = pipeline_bow.transform(title_test)\n",
    "combined_test_bow = hstack((content_train_bow, title_test_bow))\n",
    "simple_pred_test = simple_model.predict(combined_test_bow)\n",
    "\n",
    "accuracy_simple = metrics.accuracy_score(y_test, simple_pred_test)\n",
    "f1_simple = metrics.f1_score(y_test, simple_pred_test)\n",
    "\n",
    "print(\"\\nSimple model:\")\n",
    "print(\"Test set:\")\n",
    "print(\"f1 score:\", f1_simple)\n",
    "print(\"accuracy score:\", accuracy_simple)\n",
    "make_confusion_matrix(y_test, simple_pred_test, \"Simple model\")\n",
    "\n",
    "content_train_tfidf = pipeline_tfidf.fit_transform(content_train, y_train)\n",
    "content_val_tfidf = pipeline_tfidf.transform(content_val)\n",
    "content_test_tfidf = pipeline_tfidf.transform(content_test)\n",
    "title_test_tfidf = pipeline_tfidf.transform(title_test)\n",
    "combined_test_tfidf = hstack((content_train_tfidf, title_test_tfidf))\n",
    "advanced_pred_test = advanced_model.predict(combined_test_tfidf)\n",
    "\n",
    "accuracy_advanced = metrics.accuracy_score(y_test, advanced_pred_test)\n",
    "f1_advanced = metrics.f1_score(y_test, advanced_pred_test)\n",
    "\n",
    "print(\"\\nAdvanced model:\")\n",
    "print(\"Test set:\")\n",
    "print(\"f1 score:\", f1_advanced)\n",
    "print(\"accuracy score:\", accuracy_advanced)\n",
    "make_confusion_matrix(y_test, advanced_pred_test, \"Advanced model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Models on LIAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_train_bow = pipeline_bow.fit_transform(x_train_liar, y_train_liar)\n",
    "liar_test_bow = pipeline_bow.transform(x_test_liar)\n",
    "simple_pred_liar = simple_model.predict(liar_test_bow)\n",
    "\n",
    "accuracy_simple_liar = metrics.accuracy_score(y_test_liar, simple_pred_liar)\n",
    "f1_simple_liar = metrics.f1_score(y_test_liar, simple_pred_liar)\n",
    "print(\"\\nSimple model:\")\n",
    "print(\"Liar dataset:\")\n",
    "print(\"f1 score:\", f1_simple_liar)\n",
    "print(\"accuracy score:\", accuracy_simple_liar)\n",
    "make_confusion_matrix(y_test_liar, simple_pred_liar, \"Simple model on LIAR dataset\")\n",
    "\n",
    "liar_train_tfidf = pipeline_tfidf.fit_transform(x_train_liar, y_train_liar)\n",
    "liar_test_tfidf = pipeline_tfidf.transform(x_test_liar)\n",
    "advanced_pred_liar = advanced_model.predict(liar_test_tfidf)\n",
    "\n",
    "accuracy_advanced_liar = metrics.accuracy_score(y_test_liar, advanced_pred_liar)\n",
    "f1_advanced_liar = metrics.f1_score(y_test_liar, advanced_pred_liar)\n",
    "\n",
    "print(\"\\nAdvanced model:\")\n",
    "print(\"Liar dataset:\")\n",
    "print(\"f1 score:\", f1_advanced_liar)\n",
    "print(\"accuracy score:\", accuracy_advanced_liar)\n",
    "make_confusion_matrix(y_test_liar, advanced_pred_liar, \"Advanced model on LIAR dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total time: {(time() - notebook_start_time)/3600:.2f} hours\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_enviroment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
