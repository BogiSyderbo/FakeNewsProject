{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Project\n",
    "The goal of this project is to create a fake news prediction system. Fake news is a major problem that can have serious negative effects on how people understand the world around them. You will work with a dataset containing real and fake news in order to train a simple and a more advanced classifier to solve this problem. This project covers the full Data Science pipeline, from data processing, to modelling, to visualization and interpretation.\n",
    "## Part 1 Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/katikistan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/katikistan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.lm import Vocabulary\n",
    "from functools import reduce\n",
    "from cleantext import clean\n",
    "\n",
    "def clean_text(text):\n",
    "  clean_text = re.sub(r'([A-Z][A-z]+.?) ([0-9]{1,2}?), ([0-9]{4})', '<DATE>', text)\n",
    "  clean_text = clean(clean_text,\n",
    "    lower=True,\n",
    "    no_urls=True, replace_with_url=\"<URL>\",\n",
    "    no_emails=True, replace_with_email=\"<EMAIL>\",\n",
    "    no_numbers=True, replace_with_number=\"<NUM>\",\n",
    "    no_currency_symbols=True, replace_with_currency_symbol=\"<CUR>\",\n",
    "    no_punct=True, replace_with_punct=\"\",\n",
    "    no_line_breaks=True \n",
    "  )\n",
    "  return clean_text\n",
    "def tokenize(text):\n",
    "  tokens = nltk.word_tokenize(text)\n",
    "  return tokens\n",
    "def rmv_stopwords(tokens):\n",
    "  stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "  tokens = [word for word in tokens if word not in stop_words]\n",
    "  return tokens\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "  stemmer=PorterStemmer()\n",
    "  Output=[stemmer.stem(word) for word in tokens]\n",
    "  return Output\n",
    "\n",
    "# build a vocabulary from a dataframe with list of tokens\n",
    "# def build_vocabulary(df_tokens):\n",
    "#   print(type(df_tokens))\n",
    "#   tokens = \" \".join(df_tokens)\n",
    "#   return tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['awakening', 'of', '<', 'num', '>', 'strands', 'of', 'dna', 'reconnecting', 'with', 'you', 'movie', 'of', 'readers', 'think', 'this', 'story', 'is', 'fact', 'add', 'your', 'two', 'cents', 'headline', 'bitcoin', 'blockchain', 'searches', 'exceed', 'trump', 'blockchain', 'stocks', 'are', 'next', '<', 'date', '>', 'zurichtimesnet', 'as', 'miles', 'johnston', 'was', 'giving', 'update', 'it', 'was', 'another', 'case', 'of', 'strange', 'synchronicities', 'of', 'goodness', 'hidden', 'inside', 'of', 'tests', 'and', 'trials', 'like', 'a', 'follow', 'the', 'whiterabbit', 'down', 'the', 'rabbit', 'hole', 'type', 'of', 'exercise', 'in', 'researching', 'the', '<', 'num', '>', 'strands', 'of', 'dna', 'we', 'came', 'across', 'some', 'articles', 'one', 'in', 'particular', 'was', 'as', 'a', 'strange', 'synchronicity', 'written', 'exactly', '<', 'num', '>', 'year', 'ago', 'on', 'the', 'same', 'topic', '<', 'url', '>', '<', 'url', '>', 'what', 'are', 'the', '<', 'num', '>', 'strands', 'of', 'our', 'dna', 'and', 'why', 'is', 'a', 'war', 'against', 'our', 'dna', 'trailer', 'for', 'awakening', 'of', '<', 'num', '>', 'strands', 'the', 'full', 'video', 'is', 'only', 'available', 'as', 'a', 'paid', 'video', 'on', 'vimeo', 'awakening', 'of', '<', 'num', '>', 'strands', 'reconnecting', 'with', 'you', 'vimeocomondemandawakeningof12strands', 'awakening', 'of', '<', 'num', '>', 'strands', 'reconnecting', 'with', 'you', 'from', 'sandra', 'daroy', 'on', 'vimeo', 'we', 'have', 'not', 'watched', 'the', 'full', 'video', 'but', 'based', 'on', 'the', 'strange', 'synchronicities', 'present', 'within', 'we', 'suggest', 'you', 'use', 'your', 'discernment', 'as', 'always', 'necessary', 'in', 'these', 'end', 'times', '<', 'url', '>']\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('news_sample.csv')\n",
    "clean_df = df.copy()\n",
    "\n",
    "clean_df.content = clean_df.content.apply(clean_text)\n",
    "# clean_df[\"tokenized\"] = clean_df.content.apply(process_text)\n",
    "clean_df[\"tokenized\"] = clean_df.content.apply(tokenize)\n",
    "# clean_df.tokenized = clean_df.tokenized.apply(rmv_stopwords)\n",
    "# clean_df.tokenized = clean_df.tokenized.apply(stem_tokens)\n",
    "\n",
    "print(clean_df.tokenized[1])\n",
    "  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_enviroment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
