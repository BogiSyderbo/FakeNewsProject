{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake News Project\n",
    "The goal of this project is to create a fake news prediction system. Fake news is a major problem that can have serious negative effects on how people understand the world around them. You will work with a dataset containing real and fake news in order to train a simple and a more advanced classifier to solve this problem. This project covers the full Data Science pipeline, from data processing, to modelling, to visualization and interpretation.\n",
    "\n",
    "We ran the notebook with the following specs:\n",
    "- CPU: Intel(R) Xeon(R) CPU E5-2687W v3 @ 3.10GHz\n",
    "- Cores: 10\n",
    "- Threads: 20\n",
    "- Memory: 64GB Ram\n",
    "# Part 1 Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas is used to process The fake news corpus. Since content will be used for our models we drop any rows that don't have any content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd \n",
    "\n",
    "notebook_start_time = time()\n",
    "df = pd.read_csv(\"news_sample.csv\")\n",
    "dfcpy = df.copy()\n",
    "dfcpy = dfcpy.dropna(subset=['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've implemented data processing functions to do the following:\n",
    "- Clean the text\n",
    "- Tokenize the text\n",
    "- Remove stopwords\n",
    "- Remove word variations with stemming\n",
    "\n",
    "We use nltk because it has built-in support for many of these operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize.regexp import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from cleantext import clean\n",
    "\n",
    "def clean_text(text):\n",
    "  clean_text = re.sub(r'([A-Z][A-z]+.?) ([0-9]{1,2}?), ([0-9]{4})', '<DATE>', text)\n",
    "  clean_text = clean(clean_text,\n",
    "    lower=True,\n",
    "    no_urls=True, replace_with_url=\"<URL>\",\n",
    "    no_emails=True, replace_with_email=\"<EMAIL>\",\n",
    "    no_numbers=True, replace_with_number= r\"<NUM>\",\n",
    "    no_currency_symbols=True, replace_with_currency_symbol=\"<CUR>\",\n",
    "    no_punct=True, replace_with_punct=\"\",\n",
    "    no_line_breaks=True \n",
    "  )\n",
    "  return clean_text\n",
    "\n",
    "def rmv_stopwords(tokens):\n",
    "  stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "  tokens = [word for word in tokens if word not in stop_words]\n",
    "  return tokens\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "  stemmer=PorterStemmer()\n",
    "  Output=[stemmer.stem(word) for word in tokens]\n",
    "  return Output\n",
    "\n",
    "def build_vocabulary(df_tokens):\n",
    "  tokens = []\n",
    "  for lst in df_tokens:\n",
    "    tokens += lst\n",
    "  token_counter = Counter(tokens)\n",
    "  return token_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning:\n",
      "vocabulary size: 16577\n",
      "\n",
      "After removing stopwords:\n",
      "vocabulary size: 16445\n",
      "reduction rate of the vocabulary size: 0.80%\n",
      "\n",
      "After stemming:\n",
      "vocabulary size: 11031\n",
      "reduction rate of the vocabulary size: 32.92%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfcpy = df.copy()\n",
    "\n",
    "dfcpy.content = dfcpy.content.apply(clean_text)\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'<[\\w]+>|[\\w]+')\n",
    "dfcpy[\"tokenized\"] = dfcpy.content.apply(tokenizer.tokenize)\n",
    "\n",
    "vocab = build_vocabulary(dfcpy.tokenized)\n",
    "vocab_size = len(vocab)\n",
    "print(\"After cleaning:\")\n",
    "print(f\"vocabulary size: {vocab_size}\\n\")\n",
    "\n",
    "dfcpy.tokenized = dfcpy.tokenized.apply(rmv_stopwords)\n",
    "vocab = build_vocabulary(dfcpy.tokenized)\n",
    "# reduction rate of the vocabulary size\n",
    "reduction = ((vocab_size - len(vocab))/vocab_size)*100\n",
    "vocab_size = len(vocab)\n",
    "print(\"After removing stopwords:\")\n",
    "print(f\"vocabulary size: {vocab_size}\")\n",
    "print(f\"reduction rate of the vocabulary size: {reduction:.2f}%\\n\")\n",
    "\n",
    "dfcpy.tokenized = dfcpy.tokenized.apply(stem_tokens)\n",
    "vocab = build_vocabulary(dfcpy.tokenized)\n",
    "reduction = ((vocab_size - len(vocab))/vocab_size)*100\n",
    "vocab_size = len(vocab)\n",
    "print(\"After stemming:\")\n",
    "print(f\"vocabulary size: {vocab_size}\")\n",
    "print(f\"reduction rate of the vocabulary size: {reduction:.2f}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply our data processing pipeline from task 1 on the *995k FakeNewsCorpus*. \n",
    "\n",
    "Pandas is slow when used on bigger amounts of data, this is because it dosen't allow for multithreading. Modin and ray are libaries that optimize pandas by allowing pandas to run on all cores, thereby giving a speed up for the data processing. By using modin with ray as an engine you can use pandas as usual, but have it use all threads in the CPU. We used a Intel Xeon cpu with 20 threads and therefore saw huge performance gain by using modin. \n",
    "\n",
    "Modin and ray can be installed by running the following command:\n",
    "pip install \"modin[ray]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modin.config as modin_cfg\n",
    "modin_cfg.Engine.put(\"ray\") # make sure to use Ray engine and other than could be installed\n",
    "import modin.pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# only read the columns we need\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m995,000_rows.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                 \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdomain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mc\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m dfcpy \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      7\u001b[0m dfcpy \u001b[38;5;241m=\u001b[39m dfcpy\u001b[38;5;241m.\u001b[39mdropna(subset\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/utils.py:496\u001b[0m, in \u001b[0;36mexpanduser_path_arg.<locals>.decorator.<locals>.wrapped\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    494\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(patharg, Path):\n\u001b[1;32m    495\u001b[0m         params\u001b[38;5;241m.\u001b[39marguments[argname] \u001b[38;5;241m=\u001b[39m patharg\u001b[38;5;241m.\u001b[39mexpanduser()\n\u001b[0;32m--> 496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/logging/logger_decorator.py:125\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03mAny\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LogMode\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m logger \u001b[38;5;241m=\u001b[39m get_logger()\n\u001b[1;32m    128\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(log_level, start_line)\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/pandas/io.py:226\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    224\u001b[0m _, _, _, f_locals \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetargvalues(inspect\u001b[38;5;241m.\u001b[39mcurrentframe())\n\u001b[1;32m    225\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m f_locals\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m _pd_read_csv_signature}\n\u001b[0;32m--> 226\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/pandas/io.py:116\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodin\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecution\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdispatching\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfactories\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdispatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FactoryDispatcher\n\u001b[1;32m    115\u001b[0m squeeze \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqueeze\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 116\u001b[0m pd_obj \u001b[38;5;241m=\u001b[39m \u001b[43mFactoryDispatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;66;03m# This happens when `read_csv` returns a TextFileReader object for iterating through\u001b[39;00m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pd_obj, TextFileReader):\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/core/execution/dispatching/factories/dispatcher.py:197\u001b[0m, in \u001b[0;36mFactoryDispatcher.read_csv\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;129m@_inherit_docstrings\u001b[39m(factories\u001b[38;5;241m.\u001b[39mBaseFactory\u001b[38;5;241m.\u001b[39m_read_csv)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_csv\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/core/execution/dispatching/factories/factories.py:232\u001b[0m, in \u001b[0;36mBaseFactory._read_csv\u001b[0;34m(cls, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m    226\u001b[0m     _doc_io_method_template,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m )\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_csv\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/logging/logger_decorator.py:125\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03mAny\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LogMode\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m logger \u001b[38;5;241m=\u001b[39m get_logger()\n\u001b[1;32m    128\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(log_level, start_line)\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/core/io/file_dispatcher.py:164\u001b[0m, in \u001b[0;36mFileDispatcher.read\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msingle_worker_read(fname, \u001b[38;5;241m*\u001b[39margs, reason\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(err), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# TextFileReader can also be returned from `_read`.\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m AsyncReadMode\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mquery_compiler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdtypes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# at the moment it is not possible to use `wait_partitions` function;\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# in a situation where the reading function is called in a row with the\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# same parameters, `wait_partitions` considers that we have waited for\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# the end of remote calculations, however, when trying to materialize the\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# received data, it is clear that the calculations have not yet ended.\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# for example, `test_io_exp.py::test_read_evaluated_dict` is failed because of that.\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m# see #5944 for details\u001b[39;00m\n\u001b[1;32m    172\u001b[0m     _ \u001b[38;5;241m=\u001b[39m query_compiler\u001b[38;5;241m.\u001b[39mdtypes\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query_compiler\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/core/storage_formats/pandas/query_compiler.py:321\u001b[0m, in \u001b[0;36mPandasQueryCompiler.dtypes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdtypes\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modin_frame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtypes\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/core/dataframe/pandas/dataframe/dataframe.py:388\u001b[0m, in \u001b[0;36mPandasDataframe.dtypes\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;124;03mCompute the data types if they are not cached.\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;124;03m    A pandas Series containing the data types for this dataframe.\u001b[39;00m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_dtypes_cache:\n\u001b[0;32m--> 388\u001b[0m     dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    390\u001b[0m     dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtypes()\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/core/dataframe/pandas/metadata/dtypes.py:932\u001b[0m, in \u001b[0;36mModinDtypes.get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_materialized:\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value):\n\u001b[0;32m--> 932\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    934\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_value \u001b[38;5;241m=\u001b[39m pandas\u001b[38;5;241m.\u001b[39mSeries([])\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/core/io/text/text_file_dispatcher.py:945\u001b[0m, in \u001b[0;36mTextFileDispatcher._get_new_qc.<locals>.<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    903\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    904\u001b[0m \u001b[38;5;124;03mGet new query compiler from data received from workers.\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    933\u001b[0m \u001b[38;5;124;03m    New query compiler, created from `new_frame`.\u001b[39;00m\n\u001b[1;32m    934\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    935\u001b[0m partition_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_partition(\n\u001b[1;32m    936\u001b[0m     partition_ids, [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(index_ids), column_widths\n\u001b[1;32m    937\u001b[0m )\n\u001b[1;32m    939\u001b[0m new_frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mframe_cls(\n\u001b[1;32m    940\u001b[0m     partition_ids,\n\u001b[1;32m    941\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_define_index(index_ids, index_name),\n\u001b[1;32m    942\u001b[0m     column_names,\n\u001b[1;32m    943\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    944\u001b[0m     column_widths,\n\u001b[0;32m--> 945\u001b[0m     dtypes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dtypes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtypes_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    946\u001b[0m )\n\u001b[1;32m    947\u001b[0m new_query_compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mquery_compiler_cls(new_frame)\n\u001b[1;32m    948\u001b[0m skipfooter \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipfooter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/logging/logger_decorator.py:125\u001b[0m, in \u001b[0;36menable_logging.<locals>.decorator.<locals>.run_and_log\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;124;03mCompute function with logging if Modin logging is enabled.\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03mAny\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m LogMode\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mobj\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m logger \u001b[38;5;241m=\u001b[39m get_logger()\n\u001b[1;32m    128\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(log_level, start_line)\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/core/storage_formats/pandas/parsers.py:248\u001b[0m, in \u001b[0;36mPandasParser.get_dtypes\u001b[0;34m(cls, dtypes_ids, columns)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;66;03m# each element in `partitions_dtypes` is a Series, where column names are\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;66;03m# used as index and types of columns for different partitions are used as values\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m partitions_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaterialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtypes_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m([\u001b[38;5;28mlen\u001b[39m(dtype) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m dtype \u001b[38;5;129;01min\u001b[39;00m partitions_dtypes]):\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/modin/core/execution/ray/common/engine_wrapper.py:129\u001b[0m, in \u001b[0;36mRayWrapper.materialize\u001b[0;34m(cls, obj_id)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ray\u001b[38;5;241m.\u001b[39mget(obj_id) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj_id, RayObjectRefTypes) \u001b[38;5;28;01melse\u001b[39;00m obj_id\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(obj, RayObjectRefTypes) \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m obj_id):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m ids \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    132\u001b[0m result \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/ray/_private/auto_init_hook.py:21\u001b[0m, in \u001b[0;36mwrap_auto_init.<locals>.auto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(fn)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mauto_init_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     20\u001b[0m     auto_init_ray()\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/ray/_private/client_mode_hook.py:103\u001b[0m, in \u001b[0;36mclient_mode_hook.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m is_client_mode_enabled_by_default:\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(ray, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/ray/_private/worker.py:2667\u001b[0m, in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   2661\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2662\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid type of object refs, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(object_refs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, is given. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2663\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mobject_refs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must either be an ObjectRef or a list of ObjectRefs. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2664\u001b[0m     )\n\u001b[1;32m   2666\u001b[0m \u001b[38;5;66;03m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[39;00m\n\u001b[0;32m-> 2667\u001b[0m values, debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[43mworker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2668\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(values):\n\u001b[1;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, RayError):\n",
      "File \u001b[0;32m~/miniconda3/envs/fake2/lib/python3.10/site-packages/ray/_private/worker.py:843\u001b[0m, in \u001b[0;36mWorker.get_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    838\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to call `get` on the value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobject_ref\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    839\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhich is not an ray.ObjectRef.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    840\u001b[0m         )\n\u001b[1;32m    842\u001b[0m timeout_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(timeout \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m1000\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 843\u001b[0m data_metadata_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcore_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_objects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobject_refs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_task_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout_ms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    848\u001b[0m debugger_breakpoint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    849\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data, metadata \u001b[38;5;129;01min\u001b[39;00m data_metadata_pairs:\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:3483\u001b[0m, in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpython/ray/_raylet.pyx:570\u001b[0m, in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# only read the columns we need\n",
    "df = pd.read_csv(\"995,000_rows.csv\", \n",
    "                 usecols=['content', 'type', 'title', 'domain'], \n",
    "                 engine='c', \n",
    "                 dtype = str)\n",
    "dfcpy = df.copy()\n",
    "dfcpy = dfcpy.dropna(subset=['content'])\n",
    "dfcpy = dfcpy.dropna(subset=['type'])\n",
    "dfcpy = dfcpy.dropna(subset=['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "dfcpy.title = dfcpy.title.apply(clean_text)\n",
    "dfcpy.content = dfcpy.content.apply(clean_text)\n",
    "print(f\"time to clean the data: {time() - start:.2f} sec\")\n",
    "\n",
    "t = time()\n",
    "tokenizer = RegexpTokenizer(r'<[\\w]+>|[\\w]+')\n",
    "dfcpy.title = dfcpy.title.apply(tokenizer.tokenize)\n",
    "dfcpy.content = dfcpy.content.apply(tokenizer.tokenize)\n",
    "print(f\"time to tokenize the data: {(time() - t)/60:.2f} min\" )\n",
    "\n",
    "t = time()\n",
    "dfcpy.title = dfcpy.title.apply(rmv_stopwords)\n",
    "dfcpy.content = dfcpy.content.apply(rmv_stopwords)\n",
    "print(f\"time to remove stopwords: {(time() - t)/60:.2f} min\")\n",
    "\n",
    "t = time()\n",
    "dfcpy.title = dfcpy.title.apply(stem_tokens)\n",
    "dfcpy.content = dfcpy.content.apply(stem_tokens)\n",
    "print(f\"time to stem the data: {(time() - t)/60:.2f} sec\")\n",
    "\n",
    "print(f\"total time: {(time() - start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've explored the dataset and made some observations which are used to determine importance of certain metadata in the fake news corpus, such observations are:\n",
    "\n",
    "- The amount of numerics in the dataset\n",
    "- The 100 most frequent words\n",
    "- The 20 most frequent domains and how their articles are classified in terms of type\n",
    "- The distrubtion of types in the dataset\n",
    "- The amount of rows missing content, title or type (amount of rows that will be dropped from the dataset). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "vocab_content = build_vocabulary(dfcpy.content)\n",
    "print(f\"time to build vocabulary for content: {(time() - start)/60:.2f} min\")\n",
    "\n",
    "start = time()\n",
    "vocab_title = build_vocabulary(dfcpy.title)\n",
    "print(f\"time to build vocabulary for title: {(time() - start)/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# plot the frequency of the top n words\n",
    "def plot_freq(counter, top_n):\n",
    "  common_words = counter.most_common(top_n)\n",
    "\n",
    "  all_freq = {}\n",
    "  for word, freq in common_words:\n",
    "    all_freq[word] = freq\n",
    "\n",
    "  plt.figure(figsize = (top_n*0.1, 5))\n",
    "  plt.xticks(rotation = 90,fontsize = 5)\n",
    "  sns.lineplot(x = list(all_freq.keys()), y = list(all_freq.values()), color = 'red')\n",
    "  sns.barplot(x = list(all_freq.keys()), y = list(all_freq.values()))\n",
    "  plt.title(f'Top {top_n} most common words')\n",
    "  plt.xlabel('Words')\n",
    "  plt.ylabel('Frequency')\n",
    "  plt.grid(axis = 'y')\n",
    "  plt.show()\n",
    "  return\n",
    "\n",
    "def plot_domain_with_type(df):\n",
    "  top_domains = df.domain.value_counts().head(20).index\n",
    "  df = df[df.domain.isin(top_domains)]\n",
    "  df = df.groupby(['domain', 'type']).size().unstack().fillna(0)\n",
    "\n",
    "  df.plot(kind='bar', stacked=True, figsize=(10,5), title='Domain distribution with types')\n",
    "  plt.show()\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 100 most frequent words\n",
    "print(\"numerics in content: \", vocab_content[\"<num>\"])\n",
    "plot_freq(vocab_content, 100)\n",
    "print(\"numerics in titles: \", vocab_title[\"<num>\"])\n",
    "plot_freq(vocab_title, 100)\n",
    "\n",
    "# top 20 domains with their types\n",
    "plot_domain_with_type(dfcpy)\n",
    "\n",
    "# pie chart for the distribution of the types\n",
    "dfcpy.type.value_counts().plot.pie(autopct='%1.1f%%', figsize=(10,5), title='Types distribution')\n",
    "plt.show()\n",
    "\n",
    "# ammount of dropped rows\n",
    "print(f\"Number of dropped rows: {df.shape[0] - dfcpy.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exporting the cleaned dataset we have to make sure the tokens are stored correctly in the csv. A csv can correctly store a python list, therefore we store the tokens as a string using space as a seperator for each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcpy.content = dfcpy.content.apply(lambda x: ' '.join(x))\n",
    "dfcpy.title = dfcpy.title.apply(lambda x: ' '.join(x))\n",
    "dfcpy.to_csv('cleaned_news.csv', index=False)\n",
    "print(\"done cleaning the data\")\n",
    "\n",
    "# shutdown the ray engine to free up the memory\n",
    "import ray \n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the types we label articles as either fake or reliable. Some article types are omitted since it's ambigious wheter they are fake news or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('cleaned_news.csv', usecols=['content', 'type', 'title'], engine='c', dtype = str)\n",
    "dfcpy = df.copy()\n",
    "\n",
    "# label is 1 if the article is fake, 0 if the article is reliable\n",
    "dfcpy['label'] = dfcpy['type'].map({'fake': 1, \n",
    "                                    'conspiracy': 1, \n",
    "                                    'junksci': 1, \n",
    "                                    'bias': 1, \n",
    "                                    'clickbait': 0, \n",
    "                                    'political': 0, \n",
    "                                    'reliable': 0})\n",
    "dfcpy = dfcpy.dropna(subset=['label'])\n",
    "dfcpy['label'] = dfcpy['label'].astype(int)\n",
    "\n",
    "dfcpy = dfcpy.dropna(subset=['content'])\n",
    "dfcpy = dfcpy.dropna(subset=['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset into a random 80/10/10 split where 80% is used for training. 10% is used for validation and 10% is used for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Splitting the data into training (80%) and the rest (20%)\n",
    "train_df, rest_df = train_test_split(dfcpy, test_size=0.2, random_state=42, stratify=dfcpy['label'])\n",
    "# Splitting the rest into validation (50%) and test (50%)\n",
    "validation_df, test_df = train_test_split(rest_df, t\n",
    "                                          est_size=0.5, \n",
    "                                          random_state=42, \n",
    "                                          stratify=rest_df['label'])\n",
    "\n",
    "content_train, title_train ,y_train = train_df['content'], train_df['title'], train_df['label']\n",
    "content_val, title_val, y_val = validation_df['content'], validation_df['title'], validation_df['label']\n",
    "content_test, title_test, y_test = test_df['content'], test_df['title'], test_df['label']\n",
    "\n",
    "print(\"Training Set:\")\n",
    "print(train_df.content.head())\n",
    "print(train_df.title.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the Distrubution of Fake and reliable articles to get and idea on wheter our data is balanced or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the percentage distribution of 'reliable' vs. 'fake' articles\n",
    "grouped_type = dfcpy['label'].value_counts()\n",
    "grouped_type = grouped_type / grouped_type.sum() * 100\n",
    "\n",
    "# make a bar plot with percentages on bars\n",
    "plt.bar([0, 1], grouped_type, tick_label=['Reliable', 'Fake'], color=['blue', 'red'])\n",
    "plt.text(0, grouped_type[0], f'{grouped_type[0]:.2f}%', ha='center', va='bottom')\n",
    "plt.text(1, grouped_type[1], f'{grouped_type[1]:.2f}%', ha='center', va='bottom')\n",
    "plt.xlabel('Article Type')\n",
    "plt.ylabel('Percentage')\n",
    "plt.title('Percentage Distribution of Reliable vs. Fake Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing and cleaning extra reliable articles scraped from BBC news. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_extra = pd.read_csv(\"scraped_articles.csv\", usecols=['content'])\n",
    "df_extra_cpy = df_extra.copy()\n",
    "df_extra_cpy = df_extra_cpy.dropna(subset=['content'])\n",
    "df_extra_cpy.content = df_extra_cpy.content.apply(clean_text)\n",
    "tokenizer = RegexpTokenizer(r'<[\\w]+>|[\\w]+')\n",
    "df_extra_cpy.content = df_extra_cpy.content.apply(tokenizer.tokenize)\n",
    "df_extra_cpy.content = df_extra_cpy.content.apply(rmv_stopwords)\n",
    "df_extra_cpy.content = df_extra_cpy.content.apply(stem_tokens)\n",
    "df_extra_cpy['label'] = 0\n",
    "\n",
    "\n",
    "df_extra_cpy.content = df_extra_cpy.content.apply(lambda x: ' '.join(x))\n",
    "x_train_extra = pd.concat([content_train, df_extra_cpy.content], ignore_index=True)\n",
    "y_train_extra = pd.concat([y_train, df_extra_cpy.label], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "def make_confusion_matrix(y_val, y_pred,model_name):\n",
    "    confusion_matrix = metrics.confusion_matrix(y_val, y_pred, labels=[1, 0])\n",
    "    sns.heatmap(confusion_matrix, \n",
    "                annot=True, \n",
    "                fmt='g', \n",
    "                cmap='Blues', \n",
    "                xticklabels=['real', 'fake'], \n",
    "                yticklabels=['real', 'fake'])\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title(f'{model_name}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: A simple model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use logistic regression for our simple model. The model is simple in terms of vector representation (bag of words) and lack of hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.metrics as metrics\n",
    "from scipy.sparse import hstack\n",
    "from joblib import dump\n",
    "\n",
    "vectorrizer = CountVectorizer(lowercase = False, max_features = 7000, token_pattern=r'<[\\w]+>|[\\w]+')\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', vectorrizer), \n",
    "    ('scaler', StandardScaler(with_mean=False))\n",
    "    ])\n",
    "\n",
    "model = LogisticRegression(max_iter=10000, random_state=42)\n",
    "\n",
    "# making bag of words for the content and extra data\n",
    "BoW_extra = pipeline.fit_transform(x_train_extra)\n",
    "BoW_content_val = pipeline.transform(content_val)\n",
    "\n",
    "# Model with only content, but with extra data\n",
    "model.fit(BoW_extra, y_train_extra)\n",
    "y_pred = model.predict(BoW_content_val)\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"Only content, but with extra data:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy:\", accuracy)\n",
    "\n",
    "# mkain bag of words for the content\n",
    "BoW_content_train = pipeline.fit_transform(content_train)\n",
    "BoW_content_val = pipeline.transform(content_val)\n",
    "content_test_bow = pipeline.transform(content_test)\n",
    "\n",
    "# Model with only content\n",
    "model.fit(BoW_content_train, y_train)\n",
    "y_pred = model.predict(BoW_content_val)\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"Only content:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy:\", accuracy)\n",
    "# saving the model\n",
    "dump(model, 'models/simple_model_content.joblib')\n",
    "\n",
    "# making bag of words for the title and content\n",
    "BoW_title_train = pipeline.fit_transform(title_train)\n",
    "BoW_title_val = pipeline.transform(title_val)\n",
    "BoW_combined_train = hstack((BoW_content_train, BoW_title_train))\n",
    "BoW_combined_val = hstack((BoW_content_val, BoW_title_val))\n",
    "\n",
    "# Model with content and title\n",
    "model.fit(BoW_combined_train, y_train)\n",
    "y_pred = model.predict(BoW_combined_val)\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"\\nContent and title:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy:\", accuracy)\n",
    "# saving the model\n",
    "dump(model, 'models/simple_model_combined.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Advanced model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tried 3 models: \n",
    "- LinearSVM\n",
    "- Naive bayes\n",
    "- Logistic regression (using TF-IDF and cross validation)\n",
    "\n",
    "We tried 2 vector representations:\n",
    "- TF-IDF (1, 2 and 3 grams )\n",
    "- Word embedding (word2vec)\n",
    "\n",
    "We perfrom cross validation (gridsearch) on hyper paramaters to find the best hyperparameters for each model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Linear SVC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from joblib import dump\n",
    "\n",
    "def svm(x_train, y_train, x_val, model_name):\n",
    "    svc = LinearSVC(max_iter=10000, dual=False, random_state=42) \n",
    "    parameters = dict(C=[0.001, 0.1, 1, 10], loss = ['hinge', 'squared_hinge'])\n",
    "    # Cross-validation\n",
    "    grid_search = GridSearchCV(svc, parameters, cv=3, n_jobs=-1, scoring = 'f1', pre_dispatch=3)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best Parameters for svm:\", best_params)\n",
    "\n",
    "    dump(grid_search, f'models/{model_name}.joblib')\n",
    "    \n",
    "    return grid_search.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from joblib import dump\n",
    "\n",
    "def naive_bayes(x_train, y_train, x_val, model_name):\n",
    "    nb = MultinomialNB(random_state=42)\n",
    "    parameters = dict(alpha=[0.01,0.1, 1, 10])\n",
    "    # Cross-validation\n",
    "    grid_search = GridSearchCV(nb, parameters, cv=3, n_jobs=-1, scoring = 'f1')\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best parameters for Naive Bayes model:\", best_params)\n",
    "\n",
    "    dump(grid_search, f'models/{model_name}.joblib')\n",
    "    \n",
    "    return grid_search.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Logistic regression\n",
    "We noticed our simple model performed quite well, therefore we tried to optimize hyperparameters and use n-grams to see if this would improve the simple model further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as metrics\n",
    "from joblib import dump\n",
    "from time import time\n",
    "\n",
    "def logistic_advanced(x_train, y_train, x_val, model_name):\n",
    "    time_start = time()\n",
    "    logistic = LogisticRegression(max_iter = 10000, random_state=42)\n",
    "    parameters = dict(C=[0.001, 0.1, 1, 10], solver=['sag','saga'])\n",
    "    # Cross-validation\n",
    "    grid_search = GridSearchCV(logistic, parameters, cv=3, n_jobs=-1, scoring = 'f1', pre_dispatch=3)\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    print(f\"time to train the model: {time() - time_start:.2f} sec\")\n",
    "\n",
    "    best_params = grid_search.best_params_\n",
    "    print(\"Best parameters for logistic regression model:\", best_params)\n",
    "    # saving the model\n",
    "    dump(grid_search, f'models/{model_name}.joblib')\n",
    "    \n",
    "    return grid_search.predict(x_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from scipy.sparse import hstack\n",
    "from time import time\n",
    "\n",
    "def make_TFIDF(features, ngrams, metadata):\n",
    "    time_start = time()\n",
    "    global content_test, content_train, content_val, title_test, title_train, title_val\n",
    "    global y_test, y_train, y_val\n",
    "    pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(lowercase = False, \n",
    "                                   max_features=features, \n",
    "                                   min_df = 1, \n",
    "                                   max_df= 0.9, \n",
    "                                   token_pattern=r'<[\\w]+>|[\\w]+',\n",
    "                                   ngram_range =  ngrams)),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ])\n",
    "    \n",
    "    if metadata == \"content\":\n",
    "        content_train_TFIDF = pipeline.fit_transform(content_train, y_train) \n",
    "        content_val_TFIDF = pipeline.transform(content_val)\n",
    "        content_test_TFIDF = pipeline.transform(content_test)\n",
    "        print(f\"time to make TFIDF for {metadata}: {time() - time_start:.2f} sec\")\n",
    "        return content_train_TFIDF, content_val_TFIDF, content_test_TFIDF\n",
    "    if metadata == \"title\":\n",
    "        title_train_TFIDF = pipeline.fit_transform(title_train, y_train)\n",
    "        title_val_TFIDF = pipeline.transform(title_val)\n",
    "        title_test_TFIDF = pipeline.transform(title_test)\n",
    "        print(f\"time to make TFIDF for {metadata}: {time() - time_start:.2f} sec\")\n",
    "        return title_train_TFIDF, title_val_TFIDF, title_test_TFIDF\n",
    "    if metadata == \"combined\":\n",
    "        content_train_TFIDF = pipeline.fit_transform(content_train, y_train) \n",
    "        content_val_TFIDF = pipeline.transform(content_val)\n",
    "        content_test_TFIDF = pipeline.transform(content_test)\n",
    "\n",
    "        title_train_TFIDF = pipeline.fit_transform(title_train, y_train)\n",
    "        title_val_TFIDF = pipeline.transform(title_val)\n",
    "        title_test_TFIDF = pipeline.transform(title_test)\n",
    "        \n",
    "        X_train_TFIDF = hstack((content_train_TFIDF, title_train_TFIDF))\n",
    "        X_val_TFIDF = hstack((content_val_TFIDF, title_val_TFIDF))\n",
    "        X_test_TFIDF = hstack((content_test_TFIDF, title_test_TFIDF))\n",
    "        print(f\"time to make TFIDF for {metadata}: {time() - time_start:.2f} sec\")\n",
    "        return X_train_TFIDF, X_val_TFIDF, X_test_TFIDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 gram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_TFIDF, X_val_TFIDF, X_test_TFIDF = make_TFIDF(7000, (1, 1), \"content\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm(X_train_TFIDF, y_train, X_val_TFIDF, 'svm_1gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"Support vector machine:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic regression:\")\n",
    "y_pred = logistic_advanced(X_train_TFIDF, y_train, X_val_TFIDF, 'logistic_1gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_TFIDF, X_val_TFIDF, X_test_TFIDF = make_TFIDF(None, (1, 1))\n",
    "print(\"Naive Bayes:\")\n",
    "y_pred = naive_bayes(X_train_TFIDF, y_train, X_val_TFIDF, 'naive_bayes_1gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 grams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_TFIDF, X_val_TFIDF, X_test_TFIDF = make_TFIDF(7000, (2, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm(X_train_TFIDF, y_train, X_val_TFIDF, 'svm_2gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"Support vector machine:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic regression:\")\n",
    "y_pred = logistic_advanced(X_train_TFIDF, y_train, X_val_TFIDF, 'logistic_2gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_TFIDF, X_val_TFIDF, X_test_TFIDF = make_TFIDF(None, (2, 2))\n",
    "print(\"Naive Bayes:\")\n",
    "y_pred = naive_bayes(X_train_TFIDF, y_train, X_val_TFIDF, 'naive_bayes_2gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3 grams: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_TFIDF, X_val_TFIDF, X_test_TFIDF = make_TFIDF(7000, (3, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm(X_train_TFIDF, y_train, X_val_TFIDF, 'svm_2gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"Support vector machine:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Logistic regression:\")\n",
    "y_pred = logistic_advanced(X_train_TFIDF, y_train, X_val_TFIDF, 'logistic_2gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_TFIDF, X_val_TFIDF, X_test_TFIDF = make_TFIDF(None, (3, 3))\n",
    "print(\"Naive Bayes:\")\n",
    "y_pred = naive_bayes(X_train_TFIDF, y_train, X_val_TFIDF, 'naive_bayes_2gram')\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_val, y_pred)\n",
    "f1 = metrics.f1_score(y_val, y_pred)\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a Doc2Vec model on the full dataset takes a long time and is actually not needed, it's sucfficient to train the model on a subset where we take n samples from each type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a new split using x amount of each type of article\n",
    "type_amount = 1000\n",
    "\n",
    "dfcpy_subset = dfcpy.groupby('type').head(type_amount)\n",
    "print(\"Number of articles of each type in the new dataset:\"\n",
    "        ,dfcpy['type'].value_counts())\n",
    "\n",
    "dfcpy_subset = dfcpy_subset.dropna(subset=['content'])\n",
    "dfcpy_subset = dfcpy_subset.dropna(subset=['title'])\n",
    "\n",
    "X = dfcpy_subset.content \n",
    "y = dfcpy_subset.label\n",
    "\n",
    "print(\"Training Set:\")\n",
    "print(train_df.content.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def doc2vec(X, y, size, win, epo):\n",
    "    time_start = time()\n",
    "    doc2vec_model = Doc2Vec(vector_size=size, window=win, min_count=1, epochs = epo, workers = 19)\n",
    "    tagged_data = [TaggedDocument(words = word_tokenize(doc), tags=[i]) for i, doc in enumerate(X)]\n",
    "\n",
    "    doc2vec_model.build_vocab(tagged_data)  \n",
    "    doc2vec_model.train(tagged_data, \n",
    "                        total_examples = doc2vec_model.corpus_count, \n",
    "                        epochs = doc2vec_model.epochs)\n",
    "    doc_vectors = [doc2vec_model.infer_vector(word_tokenize(doc)) for doc in X]\n",
    "\n",
    "    # scale the data\n",
    "    scaler = StandardScaler()\n",
    "    doc_vectors = scaler.fit_transform(doc_vectors)\n",
    "    X_train_D2V, X_rest_D2V, y_train_D2V, y_res_D2V = train_test_split(doc_vectors,y, \n",
    "                                                                       test_size=0.2, \n",
    "                                                                       random_state=42)\n",
    "    \n",
    "    X_val_D2V, X_test_D2V, y_val_D2V, y_test_D2V = train_test_split(X_rest_D2V, \n",
    "                                                                    y_res_D2V, \n",
    "                                                                    test_size=0.5, \n",
    "                                                                    random_state=42)\n",
    "    print(f\"time to train the model: {time() - time_start:.2f} sec\")\n",
    "    return X_train_D2V, X_val_D2V, X_test_D2V, y_train_D2V, y_val_D2V, y_test_D2V\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the document vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_D2V, X_val_D2V, X_test_D2V, y_train_D2V, y_val_D2V, y_test_D2V = doc2vec(X, y, 100, 5, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm(X_train_D2V, y_train_D2V, X_val_D2V, 'svm_D2V')\n",
    "accuracy = metrics.accuracy_score(y_val_D2V, y_pred)\n",
    "f1 = metrics.f1_score(y_val_D2V, y_pred)\n",
    "print(\"Support vector machine:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)\n",
    "make_confusion_matrix(y_val_D2V, y_pred, \"SVM with Doc2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logistic_advanced(X_train_D2V, y_train_D2V, X_val_D2V, 'logistic_D2V')\n",
    "f1 = metrics.f1_score(y_val_D2V, y_pred)\n",
    "accuracy = metrics.accuracy_score(y_val_D2V, y_pred)\n",
    "print(\"Logistic regression:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)\n",
    "make_confusion_matrix(y_val_D2V, y_pred, \"Logistic Regression with Doc2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = naive_bayes(X_train_D2V, y_train_D2V, X_val_D2V, 'naive_bayes_D2V')\n",
    "f1 = metrics.f1_score(y_val_D2V, y_pred)\n",
    "accuracy = metrics.accuracy_score(y_val_D2V, y_pred)\n",
    "print(\"Naive Bayes:\")\n",
    "print(\"f1 score:\", f1)\n",
    "print(\"accuracy score:\", accuracy)\n",
    "make_confusion_matrix(y_val_D2V, y_pred, \"Naive Bayes with Doc2Vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression is with tuned hyperparameters is slightly better than linearsvc however it takes double the amount of time to train the logistic regression model, therefore we have chosen the Support vector machine instead as our model to test and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model\n",
    "from joblib import load\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "liar_train = pd.read_csv('train.tsv', sep='\\t', header=None)\n",
    "liar_val = pd.read_csv('valid.tsv', sep='\\t', header=None)\n",
    "liar_test = pd.read_csv('test.tsv', sep='\\t', header=None)\n",
    "liar = pd.concat([liar_train, liar_val, liar_test], ignore_index=True)\n",
    "liar_cpy = liar.copy()\n",
    "\n",
    "liar_cpy[2] = liar_cpy[2].apply(clean_text)\n",
    "tokenizer = RegexpTokenizer(r'<[\\w]+>|[\\w]+')\n",
    "liar_cpy[2] = liar_cpy[2].apply(tokenizer.tokenize)\n",
    "liar_cpy[2] = liar_cpy[2].apply(rmv_stopwords)\n",
    "liar_cpy[2] = liar_cpy[2].apply(stem_tokens)\n",
    "liar_cpy[2] = liar_cpy[2].apply(lambda x: ' '.join(x))\n",
    "\n",
    "labels_used = ['pants-fire', 'false', 'mostly-true', 'true']\n",
    "liar_cpy = liar_cpy.dropna(subset=[1])\n",
    "liar_cpy = liar_cpy[liar_cpy[1].isin(labels_used)]\n",
    "liar_cpy[1] = liar_cpy[1].map({'pants-fire': 1, \n",
    "                                         'false': 1, \n",
    "                                         'mostly-true': 0, \n",
    "                                         'true': 0})\n",
    "liar_cpy = liar_cpy.dropna(subset=[2])\n",
    "\n",
    "x_train_liar, x_test_liar, y_train_liar, y_test_liar = train_test_split(liar_cpy[2], \n",
    "                                                                        liar_cpy[1], \n",
    "                                                                        test_size=0.1, \n",
    "                                                                        random_state=42)\n",
    "\n",
    "pipeline_bow = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(max_features=7000, token_pattern=r'<[\\w]+>|[\\w]+')), \n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ])\n",
    "# 3500 hvis titler er med\n",
    "pipeline_tfidf = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(lowercase = False, \n",
    "                                   max_features=7000, \n",
    "                                   min_df = 1, \n",
    "                                   max_df= 0.9, \n",
    "                                   token_pattern=r'<[\\w]+>|[\\w]+',\n",
    "                                   ngram_range =  (1, 1))),\n",
    "    ('scaler', StandardScaler(with_mean=False)),\n",
    "    ])\n",
    "\n",
    "simple_model = load('models/simple_model_combined.joblib')\n",
    "advanced_model = load('models/svm_1gram_combined.joblib')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing models with FakeNewsCorpus test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_train_bow = pipeline_bow.fit_transform(content_train, y_train)\n",
    "content_val_bow = pipeline_bow.transform(content_val)\n",
    "content_test_bow = pipeline_bow.transform(content_test)\n",
    "simple_pred_test = simple_model.predict(content_test_bow)\n",
    "\n",
    "accuracy_simple = metrics.accuracy_score(y_test, simple_pred_test)\n",
    "f1_simple = metrics.f1_score(y_test, simple_pred_test)\n",
    "\n",
    "print(\"\\nSimple model:\")\n",
    "print(\"Test set:\")\n",
    "print(\"f1 score:\", f1_simple)\n",
    "print(\"accuracy score:\", accuracy_simple)\n",
    "make_confusion_matrix(y_test, simple_pred_test, \"Simple model\")\n",
    "\n",
    "content_train_tfidf = pipeline_tfidf.fit_transform(content_train, y_train)\n",
    "content_val_tfidf = pipeline_tfidf.transform(content_val)\n",
    "content_test_tfidf = pipeline_tfidf.transform(content_test)\n",
    "advanced_pred_test = advanced_model.predict(content_test_tfidf)\n",
    "\n",
    "accuracy_advanced = metrics.accuracy_score(y_test, advanced_pred_test)\n",
    "f1_advanced = metrics.f1_score(y_test, advanced_pred_test)\n",
    "\n",
    "print(\"\\nAdvanced model:\")\n",
    "print(\"Test set:\")\n",
    "print(\"f1 score:\", f1_advanced)\n",
    "print(\"accuracy score:\", accuracy_advanced)\n",
    "make_confusion_matrix(y_test, advanced_pred_test, \"Advanced model\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Content + titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_train_bow = pipeline_bow.fit_transform(content_train, y_train)\n",
    "content_val_bow = pipeline_bow.transform(content_val)\n",
    "content_test_bow = pipeline_bow.transform(content_test)\n",
    "title_test_bow = pipeline_bow.transform(title_test)\n",
    "combined_test_bow = hstack((content_train_bow, title_test_bow))\n",
    "simple_pred_test = simple_model.predict(combined_test_bow)\n",
    "\n",
    "accuracy_simple = metrics.accuracy_score(y_test, simple_pred_test)\n",
    "f1_simple = metrics.f1_score(y_test, simple_pred_test)\n",
    "\n",
    "print(\"\\nSimple model:\")\n",
    "print(\"Test set:\")\n",
    "print(\"f1 score:\", f1_simple)\n",
    "print(\"accuracy score:\", accuracy_simple)\n",
    "make_confusion_matrix(y_test, simple_pred_test, \"Simple model\")\n",
    "\n",
    "content_train_tfidf = pipeline_tfidf.fit_transform(content_train, y_train)\n",
    "content_val_tfidf = pipeline_tfidf.transform(content_val)\n",
    "content_test_tfidf = pipeline_tfidf.transform(content_test)\n",
    "title_test_tfidf = pipeline_tfidf.transform(title_test)\n",
    "combined_test_tfidf = hstack((content_train_tfidf, title_test_tfidf))\n",
    "advanced_pred_test = advanced_model.predict(combined_test_tfidf)\n",
    "\n",
    "accuracy_advanced = metrics.accuracy_score(y_test, advanced_pred_test)\n",
    "f1_advanced = metrics.f1_score(y_test, advanced_pred_test)\n",
    "\n",
    "print(\"\\nAdvanced model:\")\n",
    "print(\"Test set:\")\n",
    "print(\"f1 score:\", f1_advanced)\n",
    "print(\"accuracy score:\", accuracy_advanced)\n",
    "make_confusion_matrix(y_test, advanced_pred_test, \"Advanced model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Models on LIAR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_train_bow = pipeline_bow.fit_transform(x_train_liar, y_train_liar)\n",
    "liar_test_bow = pipeline_bow.transform(x_test_liar)\n",
    "simple_pred_liar = simple_model.predict(liar_test_bow)\n",
    "\n",
    "accuracy_simple_liar = metrics.accuracy_score(y_test_liar, simple_pred_liar)\n",
    "f1_simple_liar = metrics.f1_score(y_test_liar, simple_pred_liar)\n",
    "print(\"\\nSimple model:\")\n",
    "print(\"Liar dataset:\")\n",
    "print(\"f1 score:\", f1_simple_liar)\n",
    "print(\"accuracy score:\", accuracy_simple_liar)\n",
    "make_confusion_matrix(y_test_liar, simple_pred_liar, \"Simple model on LIAR dataset\")\n",
    "\n",
    "liar_train_tfidf = pipeline_tfidf.fit_transform(x_train_liar, y_train_liar)\n",
    "liar_test_tfidf = pipeline_tfidf.transform(x_test_liar)\n",
    "advanced_pred_liar = advanced_model.predict(liar_test_tfidf)\n",
    "\n",
    "accuracy_advanced_liar = metrics.accuracy_score(y_test_liar, advanced_pred_liar)\n",
    "f1_advanced_liar = metrics.f1_score(y_test_liar, advanced_pred_liar)\n",
    "\n",
    "print(\"\\nAdvanced model:\")\n",
    "print(\"Liar dataset:\")\n",
    "print(\"f1 score:\", f1_advanced_liar)\n",
    "print(\"accuracy score:\", accuracy_advanced_liar)\n",
    "make_confusion_matrix(y_test_liar, advanced_pred_liar, \"Advanced model on LIAR dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"total time: {(time() - notebook_start_time)/3600:.2f} hours\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_enviroment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
